# Development Environment Variables
# Copy this to .env.local (never commit .env.local)

# Node environment
NODE_ENV=development

# Application environment (controls adapter wiring)
APP_ENV=production
# APP_ENV=test  # Uncomment to use fake adapters locally (CI sets this automatically)

# =============================================================================
# Next.js App
# =============================================================================

# App runtime (used by Next.js server/runtime only)
APP_BASE_URL=http://localhost:3000
NEXTAUTH_URL=http://localhost:3000
DOMAIN=localhost

# ── Database ──────────────────────────────────────────────────────────────────
# Per DATABASE_RLS_SPEC.md design decision 7: runtime app consumes explicit DSNs only.
# Both DSNs required. Users MUST be distinct (RLS enforcement requires role separation).
# Run `pnpm docker:stack:setup` first to create app_user + app_service roles via provision.sh.
#
# app_user role (RLS enforced) — used by Next.js request paths
DATABASE_URL="postgresql://app_user:password@localhost:55432/cogni_template_dev"
# app_service role (BYPASSRLS) — used by auth, workers, bootstrap
DATABASE_SERVICE_URL="postgresql://app_service:service_password@localhost:55432/cogni_template_dev"

# ── Docker stack provisioning (docker:stack:setup only) ───────────────────────
# These feed provision.sh which creates app_user + app_service roles.
# Not used by the Next.js app directly — only consumed by db-provision container.
POSTGRES_ROOT_USER=postgres
POSTGRES_ROOT_PASSWORD=postgres
APP_DB_NAME=cogni_template_dev
APP_DB_USER=app_user
APP_DB_PASSWORD=password
APP_DB_SERVICE_USER=app_service
APP_DB_SERVICE_PASSWORD=service_password

# Auth.js (NextAuth) - Required for authentication
# Generate with: openssl rand -base64 32
AUTH_SECRET="local-dev-secret-at-least-32-characters-long-change-me"

# LLM Integration (Stage 8) - LITELLM_BASE_URL auto-detects based on NODE_ENV
# Default models are computed from LiteLLM catalog metadata (metadata.cogni.default_preferred/default_free)
LITELLM_BASE_URL=http://localhost:4000
LITELLM_MASTER_KEY=your-litellm-master-key-here
OPENROUTER_API_KEY=your-openrouter-api-key-here

# OpenClaw Gateway (sandbox-openclaw profile)
# Auth token for gateway WS handshake — must match openclaw-gateway.json gateway.auth.token
# Generate with: openssl rand -base64 32
OPENCLAW_GATEWAY_TOKEN=your-openclaw-gateway-token-min32ch
# Git relay (host-side push + PR). Requires Contents:Write + Pull requests:Write.
OPENCLAW_GITHUB_RW_TOKEN=ghp_your-github-rw-token-here
# Discord bot token for OpenClaw gateway channel
# Get from: discord.com/developers/applications → your app → Bot → Reset Token
DISCORD_BOT_TOKEN=your-discord-bot-token-here

# LangGraph Dev Server (Optional)
# Set to enable langgraph dev server execution instead of in-process.
# Start server with: pnpm langgraph:dev
# Default port for langgraph dev: 2024
# LANGGRAPH_DEV_URL=http://localhost:2024

# LangSmith API Key (only for langgraph dev server, not loaded by Next.js)
# Required by LangGraph Studio/dev server for tracing.
# Get from: https://smith.langchain.com → Settings → API Keys
# LANGSMITH_API_KEY=lsv2_pt_your-api-key-here

# Optional server config
PORT=3000
PINO_LOG_LEVEL=debug

# ── Observability ─────────────────────────────────────────────────────────────

# Log Collection (Alloy → Loki)
# For local dev: Uses local Loki (no cloud credentials needed, auto-configured by docker-compose.dev.yml)
# DEPLOY_ENVIRONMENT=local  # Auto-set by docker-compose.dev.yml, no need to define
# LOKI_WRITE_URL=http://loki:3100/loki/api/v1/push  # Auto-set by docker-compose.dev.yml
# LOKI_USERNAME=  # Empty for local dev (auto-set by docker-compose.dev.yml)
# LOKI_PASSWORD=  # Empty for local dev (auto-set by docker-compose.dev.yml)

# Optional: Override to send local logs to Grafana Cloud instead (not recommended for dev)
# DEPLOY_ENVIRONMENT=local
# LOKI_WRITE_URL="https://logs-prod-020.grafana.net/loki/api/v1/push"
# LOKI_USERNAME="123456"  # Your numeric user ID
# LOKI_PASSWORD="glc_your_api_key_here"  # API key with logs:write permission

# Prometheus Metrics (Stage 9)
# Token for /api/metrics endpoint authentication (min 32 chars, required in production)
METRICS_TOKEN=local-dev-metrics-token-32chars!

# Optional: Prometheus metrics (Grafana Cloud)
# WRITE path (Alloy remote_write) - write-only token
# PROMETHEUS_REMOTE_WRITE_URL="https://prometheus-prod-37-prod-ap-southeast-1.grafana.net/api/prom/push"
# PROMETHEUS_USERNAME="123456"  # Your numeric user ID
# PROMETHEUS_PASSWORD="glc_write_token_here"  # API key with metrics:write ONLY

# READ path (app queries) - read-only token (separate from write for least privilege)
# PROMETHEUS_QUERY_URL is auto-derived from PROMETHEUS_REMOTE_WRITE_URL (strips /push)
# PROMETHEUS_READ_USERNAME="123456"  # Same user ID is fine
# PROMETHEUS_READ_PASSWORD="glc_..."  # Create at https://grafana.com/orgs/<org>/access-policies → metrics:read scope

# Grafana Observability (required for governance agents)
# Used by: MCP tools (log queries) + grafana-health skill (metrics/alerts)
# For local dev: Use grafana-local MCP (http://localhost:3001) or cloud (https://your-org.grafana.net)
# Generate token: Grafana → Administration → Service accounts → Add token (Viewer role)
GRAFANA_URL="https://your-org.grafana.net"
GRAFANA_SERVICE_ACCOUNT_TOKEN="glsa_your_service_account_token_here"

# Langfuse - AI Observability (Optional)
# When set, AI completions are traced to Langfuse for debugging and eval.
# Get keys from: https://cloud.langfuse.com → Settings → API Keys
# LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key
# LANGFUSE_SECRET_KEY=sk-lf-your-secret-key
# LANGFUSE_BASE_URL=https://cloud.langfuse.com  # Optional, defaults to cloud. Use for self-hosted.

# ── Internal API tokens ───────────────────────────────────────────────────────
# These tokens authenticate service-to-service calls. All min 32 chars.

# Scheduler API token - auth for scheduler-worker → internal graph execution API
# Used by scheduler-worker to call POST /api/internal/graphs/{graphId}/runs
# Generate with: openssl rand -base64 32
SCHEDULER_API_TOKEN=local-dev-scheduler-api-token-min32chars

# Billing ingest token - auth for LiteLLM generic_api callback → billing ingest endpoint
# Used by LiteLLM to call POST /api/internal/billing/ingest
BILLING_INGEST_TOKEN=local-dev-billing-ingest-token-min32ch

# Internal ops token - auth for deploy-time governance sync trigger endpoint
# Used by deploy helpers to call POST /api/internal/ops/governance/schedules/sync
INTERNAL_OPS_TOKEN=local-dev-internal-ops-token-min32ch

# Billing ingest callback URL - how LiteLLM (in Docker) reaches the app's ingest endpoint
# For pnpm dev:stack (app runs natively on host): use host.docker.internal
# For pnpm docker:stack (app runs in Docker): leave unset (defaults to http://app:3000/...)
GENERIC_LOGGER_ENDPOINT=http://host.docker.internal:3000/api/internal/billing/ingest

# =============================================================================
# Scheduler Worker (services/scheduler-worker)
# =============================================================================

# Temporal Schedule Orchestration (required)
# App requires Temporal - start with: pnpm dev:infra
# Use localhost for host-mode dev (pnpm dev); docker-compose overrides for container-mode
TEMPORAL_ADDRESS=localhost:7233
TEMPORAL_NAMESPACE=cogni-production
TEMPORAL_TASK_QUEUE=scheduler-tasks

# Temporal DB (compose-only, app never uses these):
# TEMPORAL_DB_HOST=temporal-postgres
# TEMPORAL_DB_PORT=5432
# TEMPORAL_DB_USER=temporal
# TEMPORAL_DB_PASSWORD=temporal

# Scheduler worker app URL - how the scheduler-worker (in Docker) reaches the app
# For pnpm dev:stack (app runs natively on host): use host.docker.internal
# For pnpm docker:stack (app runs in Docker): leave unset (defaults to http://app:3000)
SCHEDULER_APP_BASE_URL=http://host.docker.internal:3000
SCHEDULER_WORKER_HEALTH_URL=http://localhost:9001

# GitHub App for ingestion (optional — only needed for GitHub activity collection)
# Create a GitHub App in your org, then set these:
# GITHUB_REVIEW_APP_ID=123456
# GITHUB_REVIEW_APP_PRIVATE_KEY_BASE64=<base64-encoded PEM>
# GITHUB_REVIEW_INSTALLATION_ID=<optional, resolved dynamically if omitted>
# GITHUB_REPOS=owner/repo1,owner/repo2

# =============================================================================
# Shared / Cross-cutting
# =============================================================================

# EVM RPC - On-chain verification (Base mainnet)
# Get free API key from: alchemy.com or infura.io
EVM_RPC_URL=https://base-mainnet.g.alchemy.com/v2/your-api-key-here

# Public client vars
# Wallet Integration
NEXT_PUBLIC_WALLETCONNECT_PROJECT_ID=get-from-walletconnect-cloud  # Get from cloud.walletconnect.com

# Crypto widget payments (DePay) - crypto-only credits
# DAO receiving wallet + chain come from .cogni/repo-spec.yaml (no env override)

# Development tokens (replace with your own)
CHERRY_AUTH_KEY=your-cherry-auth-key-here
CR_PAT=your-github-pat-here

# Brain repo access (required — no default, no silent cwd fallback)
# "." = local checkout for pnpm dev. See docs/COGNI_BRAIN_SPEC.md "Repo Access Modes".
COGNI_REPO_PATH=.

# git-sync (required for all docker workflows: dev:infra, docker:dev:stack, docker:stack)
# The git-sync container clones the repo into a shared volume; the app reads /repo/current.
# Fine-grained PAT: Contents:Read scope, scoped to this repo (public repos work without token)
COGNI_REPO_URL=https://github.com/Cogni-DAO/node-template.git
COGNI_REPO_REF=HEAD
GIT_READ_USERNAME=your-github-username
GIT_READ_TOKEN=your-fine-grained-pat-here

# Test Environment Configuration
# Used by API integration tests in tests/api/**

# For dev:stack (Next.js runs locally, containers provide infrastructure)
TEST_BASE_URL=http://localhost:3000/

# For docker:stack (everything in containers, accessible via Caddy)
# TEST_BASE_URL=https://localhost/

# Remote environments
# TEST_BASE_URL=https://your-domain.com/

# Billing Evolution Stage 6.5
USER_PRICE_MARKUP_FACTOR=2.0

# System tenant revenue share — fraction of user credits minted as bonus to system tenant
# 0 = disabled, 0.75 = 75% bonus (default). Per docs/spec/system-tenant.md
# SYSTEM_TENANT_REVENUE_SHARE=0.75
