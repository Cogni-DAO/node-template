---
id: langgraph-patterns-spec
type: spec
title: LangGraph Patterns
status: draft
spec_state: draft
trust: draft
summary: Architecture patterns and invariants for LangGraph agentic workflows across InProc and Server execution paths.
read_when: Working with LangGraph graphs, modifying AI execution pipeline, or understanding package boundaries.
implements:
owner: derekg1729
created: 2026-02-07
verified:
tags: [ai-graphs, langgraph]
---

# LangGraph Patterns

## Context

Cogni's baseline Open Source foundation for building and executing AI agent graphs is LangGraph. All LangGraph code is isolated in `packages/langgraph-graphs/`, with executor-agnostic primitives in `packages/ai-core/` and pure tool definitions in `packages/ai-tools/`. Both InProc (cogni-developed) and LanggraphServer (Langchain non-OSS) executors implement `GraphExecutorPort` for unified billing and telemetry.

## Goal

Define the package boundaries, execution paths, and invariants that govern LangGraph graph creation and execution. Ensure all AI execution flows through `GraphExecutorPort` regardless of executor choice. Custom InProc langraph executor must model as closely to LangGraph Server's I/O for graph execution as possible.

## Non-Goals

- Server infrastructure details (Docker, Redis, container deployment) â€” see [LangGraph Server](../LANGGRAPH_SERVER.md)
- Executor-agnostic billing and tracking patterns â€” see [Graph Execution](graph-execution.md)
- Step-by-step guide for adding new graphs â€” see [Agent Development Guide](../guides/agent-development.md)

## Core Invariants

1. **NO_LANGCHAIN_IN_SRC**: `src/**` cannot import `@langchain/*`. Enforced by Biome `noRestrictedImports`.

2. **PACKAGES_NO_SRC_IMPORTS**: `packages/**` cannot import from `src/**`. Enforced by dependency-cruiser.

3. **ENV_FREE_EXPORTS**: Package exports never read `env.ts` or instantiate provider SDKs directly.

4. **SINGLE_AIEVENT_CONTRACT**: Common subset: `text_delta`, `usage_report`, `assistant_final`, `done`. Tool events are InProc-only for P0.

5. **NO_AWAIT_IN_TOKEN_PATH**: Token emission â†’ AiEvent yield must not await I/O. Use synchronous queue push.

6. **SINGLE_QUEUE_PER_RUN**: Each graph run owns exactly one AsyncQueue. Tool events and LLM events flow to the same queue.

7. **ASSISTANT_FINAL_REQUIRED**: On success, emit exactly one `assistant_final` event with complete response.

8. **CATALOG_SINGLE_SOURCE_OF_TRUTH**: Catalog exported by `@cogni/langgraph-graphs`, references compiled graphs.

9. **NO_PARALLEL_REQUEST_TYPES**: Providers use `GraphRunRequest`/`GraphRunResult` from `@/ports`.

## Design

### Architecture Contract

| Category                | Status         | Notes                                                                |
| ----------------------- | -------------- | -------------------------------------------------------------------- |
| **Package structure**   | âœ… Implemented | ai-core, ai-tools, langgraph-graphs                                  |
| **Compiled exports**    | ðŸ“‹ Contract    | Graphs export `compile()` with no args                               |
| **TOOL_CATALOG**        | ðŸ“‹ Contract    | Canonical registry in `ai-tools`; wrapper checks `toolIds` allowlist |
| **ALS runtime context** | ðŸ“‹ Contract    | `getCogniExecContext()` per-run isolation                            |

> See [Graph Execution](graph-execution.md) for authoritative invariants and implementation status.

### Execution Paths

| Path       | Adapter                       | Use Case                                             |
| ---------- | ----------------------------- | ---------------------------------------------------- |
| **InProc** | `InProcCompletionUnitAdapter` | Next.js process; billing via executeCompletionUnit() |
| **Server** | `LangGraphServerAdapter`      | External LangGraph Server container                  |

All AI execution flows through `GraphExecutorPort`. The executor choice is an implementation detail behind the unified interface.

### Package Structure

```
packages/
â”œâ”€â”€ ai-core/                          # Executor-agnostic primitives (NO LangChain)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ events/ai-events.ts       # AiEvent union
â”‚       â”œâ”€â”€ usage/usage.ts            # UsageFact, ExecutorType
â”‚       â”œâ”€â”€ configurable/             # GraphRunConfig schema
â”‚       â””â”€â”€ tooling/                  # Tool execution types + runtime
â”‚           â”œâ”€â”€ types.ts              # ToolExecFn, BoundToolRuntime, EmitAiEvent
â”‚           â”œâ”€â”€ tool-runner.ts        # createToolRunner (canonical pipeline)
â”‚           â”œâ”€â”€ ai-span.ts            # AiSpanPort (observability interface)
â”‚           â””â”€â”€ runtime/tool-policy.ts # ToolPolicy, createToolAllowlistPolicy
â”‚
â”œâ”€â”€ ai-tools/                         # Pure tool definitions (NO LangChain)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ types.ts                  # ToolContract, BoundTool, ToolResult
â”‚       â”œâ”€â”€ catalog.ts                # TOOL_CATALOG: Record<string, BoundTool>
â”‚       â””â”€â”€ tools/*.ts                # Pure implementations
â”‚
â””â”€â”€ langgraph-graphs/                 # ALL LangChain code lives here
    â””â”€â”€ src/
        â”œâ”€â”€ catalog.ts                # LANGGRAPH_CATALOG (graph metadata)
        â”œâ”€â”€ graphs/                   # Graph definitions
        â”‚   â”œâ”€â”€ index.ts              # Barrel: inproc entrypoints
        â”‚   â”œâ”€â”€ poet/
        â”‚   â”‚   â”œâ”€â”€ graph.ts          # Pure factory: createPoetGraph({ llm, tools })
        â”‚   â”‚   â”œâ”€â”€ server.ts         # langgraph dev entrypoint (initChatModel)
        â”‚   â”‚   â”œâ”€â”€ cogni-exec.ts     # Cogni executor entrypoint (ALS-based)
        â”‚   â”‚   â””â”€â”€ prompts.ts        # System prompts
        â”‚   â””â”€â”€ <agent>/
        â”‚       â”œâ”€â”€ graph.ts          # Pure factory
        â”‚       â”œâ”€â”€ server.ts         # langgraph dev entrypoint
        â”‚       â”œâ”€â”€ cogni-exec.ts     # Cogni executor entrypoint
        â”‚       â””â”€â”€ prompts.ts        # System prompts
        â””â”€â”€ runtime/                  # Runtime utilities
            â”œâ”€â”€ core/                 # Generic (no ALS)
            â”‚   â”œâ”€â”€ async-queue.ts
            â”‚   â”œâ”€â”€ message-converters.ts
            â”‚   â”œâ”€â”€ langchain-tools.ts   # makeLangChainTools, toLangChainToolsCaptured
            â”‚   â””â”€â”€ server-entrypoint.ts
            â””â”€â”€ cogni/                # Cogni executor (uses ALS)
                â”œâ”€â”€ exec-context.ts      # CogniExecContext, runWithCogniExecContext
                â”œâ”€â”€ completion-adapter.ts # CogniCompletionAdapter (Runnable-based)
                â”œâ”€â”€ tools.ts             # toLangChainToolsFromContext
                â””â”€â”€ entrypoint.ts        # createCogniEntrypoint
```

**Supported import surface:**

```typescript
// Compiled graph exports
import { poetGraph, pondererGraph } from "@cogni/langgraph-graphs/graphs";

// Runtime utilities
import {
  CogniCompletionAdapter,
  toBaseMessage,
} from "@cogni/langgraph-graphs/runtime";
```

### Type Boundaries

| Type                                | Defined In             | Used By                              |
| ----------------------------------- | ---------------------- | ------------------------------------ |
| `GraphRunRequest`, `GraphRunResult` | `@/ports`              | `GraphExecutorPort`, `GraphProvider` |
| `GraphRunConfig`                    | `@cogni/ai-core`       | All adapters, graphs                 |
| `LangGraphCatalogEntry`             | `langgraph/catalog.ts` | `LangGraphInProcProvider`            |

### Persistence Integration

Persistence is handled by parallel stream subscribers â€” runner owns event emission, not storage:

| Subscriber            | Event              | Action                                      |
| --------------------- | ------------------ | ------------------------------------------- |
| **BillingSubscriber** | `usage_report`     | `commitUsageFact()` â†’ charge_receipts       |
| **HistorySubscriber** | `assistant_final`  | `persistArtifact()` â†’ run_artifacts (cache) |
| **UI Subscriber**     | `text_delta`, etc. | Forward to client (may disconnect)          |

Key contracts from [Usage History spec](./usage-history.md):

- **NO_DELTA_STORAGE**: P0 persists only user input + assistant final output. No streaming deltas.
- **ARTIFACTS_ARE_CACHE**: `run_artifacts` is best-effort transcript cache, not source of truth. For `langgraph_server`, LangGraph owns canonical thread state.
- **REDACT_BEFORE_PERSIST**: Masking applied before `content_hash` computation and storage. Single redaction boundary.
- **TENANT_SCOPED**: All artifacts include `account_id`. RLS enforces isolation. `UNIQUE(account_id, run_id, artifact_key)` for idempotency.

Runner responsibility: Emit `assistant_final` with complete content. HistoryWriter persists directly â€” no delta assembly required.

### InProc Execution Path

InProc executes LangGraph within the Next.js server runtime with billing through the adapter layer.

**Data Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AiRuntimeService.runGraph(request)                                  â”‚
â”‚ - Routes via AggregatingGraphExecutor by graphId                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangGraphInProcProvider                                             â”‚
â”‚ - Looks up compiled graph from catalog                              â”‚
â”‚ - Sets up AsyncLocalStorage context (completionFn, tokenSink)       â”‚
â”‚ - Invokes: graph.invoke(messages, { configurable })                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compiled Graph (packages/langgraph-graphs/src/graphs/*)             â”‚
â”‚ - Accesses runtime via getCogniExecContext()                        â”‚
â”‚ - LLM calls route through CogniCompletionAdapter                    â”‚
â”‚ - Tools resolved by toolIds via ToolRegistry                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**CogniCompletionAdapter** (`runtime/cogni/completion-adapter.ts`) is a `Runnable`-based wrapper that routes LLM calls through the ALS-provided `CompletionFn` for billing/streaming integration.

Key design:

- Extends `Runnable` (not `BaseChatModel`) so `configurable` is available in `invoke()`
- Model read from `config.configurable.model`
- Non-serializable deps (`completionFn`, `tokenSink`) from ALS
- Includes `_modelType()` for LangGraph duck-typing compatibility
- Fails fast if ALS context or model missing

**Runtime Context:** The provider sets up ALS context before graph invocation. Per NO_MODEL_IN_ALS (see [Graph Execution](graph-execution.md)), the runtime holds only non-serializable dependencies (`completionFn`, `tokenSink`, `toolExecFn`). Model travels via `configurable`.

### Server Execution Path

LangGraphServerAdapter calls external LangGraph Server via SDK. Server owns thread state/checkpoints and routes LLM through LiteLLM proxy. `stateKey` is required; send only new user input; server owns thread state. Tools work per-run. InProc path ignores `stateKey` (no thread persistence).

See [LangGraph Server](../LANGGRAPH_SERVER.md) for infrastructure details.

### Tool Structure

Tool schemas are bound at graph compile time. `configurable.toolIds` is a **runtime allowlist** checked at execution:

```typescript
// @cogni/ai-tools/catalog.ts - canonical registry
export const TOOL_CATALOG = {
  core__get_current_time: getCurrentTimeBoundTool,
  core__web_search: webSearchBoundTool,
};

// toLangChainTool wrapper checks allowlist at execution
func: async (args, runManager?, config?) => {
  const allowed = config?.configurable?.toolIds ?? [];
  if (!allowed.includes(toolName)) {
    return { ok: false, errorCode: "policy_denied", safeMessage: "..." };
  }
  return exec(toolName, args, config?.configurable);
};
```

| Package                   | Owns                                  | Dependencies                         |
| ------------------------- | ------------------------------------- | ------------------------------------ |
| `@cogni/ai-tools`         | `TOOL_CATALOG`, contracts, schemas    | `zod` only                           |
| `@cogni/langgraph-graphs` | `toLangChainTool` (wraps + allowlist) | `@cogni/ai-tools`, `@langchain/core` |

### langgraph.json Configuration

For Server path, graphs are registered in `packages/langgraph-server/langgraph.json`:

```json
{
  "node_version": "20",
  "graphs": {
    "chat": "./src/index.ts:chatGraph",
    "my-agent": "./src/index.ts:myAgentGraph"
  },
  "env": ".env"
}
```

The `langgraph-server` package re-exports graphs from `@cogni/langgraph-graphs/graphs`.

### Anti-Patterns

1. **No `@langchain` imports in `src/`** â€” All LangChain code in `packages/langgraph-graphs/`
2. **No hardcoded models in graphs** â€” Model comes from ALS (provider sets from `configurable.model`)
3. **No direct `ChatOpenAI` in InProc** â€” Use `CogniCompletionAdapter` wrapper for billing
4. **No tool instances in configurable** â€” Pass `toolIds`, resolve via registry
5. **No constructor args on graph exports** â€” Graphs compile with no args; runtime config via `configurable`
6. **No env reads in package exports** â€” Use `AsyncLocalStorage` context
7. **No `await` in token sink** â€” `tokenSink.push()` must be synchronous
8. **No `streamEvents()` for InProc** â€” Use `invoke()` + AsyncQueue
9. **No forked tool wrapper logic** â€” Single `makeLangChainTools` impl; thin wrappers resolve `toolExecFn` differently
10. **No constructor args on `CogniCompletionAdapter`** â€” No-arg constructor; reads model from `configurable` and deps from ALS at invoke time

### File Pointers

| File                                                                | Purpose                                   |
| ------------------------------------------------------------------- | ----------------------------------------- |
| `packages/ai-core/src/events/ai-events.ts`                          | AiEvent union type                        |
| `packages/ai-core/src/tooling/tool-runner.ts`                       | createToolRunner (canonical pipeline)     |
| `packages/ai-tools/src/catalog.ts`                                  | TOOL_CATALOG registry                     |
| `packages/langgraph-graphs/src/catalog.ts`                          | LANGGRAPH_CATALOG (graph metadata)        |
| `packages/langgraph-graphs/src/graphs/index.ts`                     | Barrel: inproc entrypoints                |
| `packages/langgraph-graphs/src/runtime/cogni/exec-context.ts`       | CogniExecContext, runWithCogniExecContext |
| `packages/langgraph-graphs/src/runtime/cogni/completion-adapter.ts` | CogniCompletionAdapter                    |
| `packages/langgraph-graphs/src/runtime/cogni/entrypoint.ts`         | createCogniEntrypoint                     |
| `packages/langgraph-graphs/src/runtime/core/server-entrypoint.ts`   | createServerEntrypoint                    |
| `packages/langgraph-graphs/langgraph.json`                          | LangGraph Server graph registration       |

## Acceptance Checks

**Automated:**

- `pnpm packages:build` â€” all three packages (ai-core, ai-tools, langgraph-graphs) build without errors
- Biome `noRestrictedImports` rule enforces NO_LANGCHAIN_IN_SRC

**Manual:**

1. Verify no `@langchain/*` imports exist in `src/` (`grep -r "@langchain" src/`)
2. Verify graph catalog entries reference compiled graphs

## Open Questions

- [ ] Stream controller "already closed" error â€” non-blocking; stream completes despite error on client disconnect
- [ ] Tool call ID architecture â€” P0 workaround generates UUID; should propagate model's `tool_call_id`

## Related

- [Agent Development Guide](../guides/agent-development.md) â€” Step-by-step for adding new agent graphs
- [Graph Execution](graph-execution.md) â€” Executor-agnostic billing, tracking, UI/UX patterns
- [LangGraph Server](../LANGGRAPH_SERVER.md) â€” Infrastructure: Docker, Redis, container deployment
- [Tool Use Spec](./tool-use.md) â€” Tool execution invariants
- [Usage History Spec](./usage-history.md) â€” Run artifacts, assistant_final persistence
- [AI Setup Spec](./ai-setup.md) â€” Correlation IDs, telemetry
