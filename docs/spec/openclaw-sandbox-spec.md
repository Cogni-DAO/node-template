---
id: openclaw-sandbox-spec
type: spec
title: OpenClaw Sandbox Integration
status: active
spec_state: draft
trust: draft
summary: OpenClaw agent runtime in Cogni — two execution modes (ephemeral sandbox, long-running gateway), invariants, container images, LLM protocol, I/O protocol, billing
read_when: Implementing OpenClaw sandbox or gateway agent, modifying container images, or debugging sandbox LLM calls
owner: derekg1729
created: 2026-02-07
verified: 2026-02-07
tags: [sandbox, openclaw, ai-agents]
---

# OpenClaw Sandbox Integration

> [!CRITICAL]
> OpenClaw runs in two modes: **ephemeral** (one-shot `network=none` container, CLI invocation) and **gateway** (long-running service on `sandbox-internal`, WS protocol). Both route LLM calls through an nginx proxy to LiteLLM — OpenClaw never touches API keys directly. OpenClaw's own sandbox and cron are **disabled** in both modes. Supersedes the former `CLAWDBOT_ADAPTER_SPEC.md`.

## Execution Modes

|                       | Ephemeral                                    | Gateway                                                                               |
| --------------------- | -------------------------------------------- | ------------------------------------------------------------------------------------- |
| **Container**         | `cogni-sandbox-openclaw:latest`              | `openclaw-outbound-headers:latest` (`ghcr.io/cogni-dao/openclaw-outbound-headers:v0`) |
| **Lifecycle**         | One-shot per run, destroyed after            | Long-running compose service, shared                                                  |
| **Network**           | `network=none` (isolated)                    | `sandbox-internal` (Docker DNS)                                                       |
| **Invocation**        | CLI: `--local --agent main --message ...`    | WS: custom frame protocol on port 18789                                               |
| **Concurrency**       | Single user per container                    | Multiple concurrent sessions                                                          |
| **LLM proxy**         | Per-run nginx via unix socket (socat)        | Shared nginx (`llm-proxy-openclaw:8080`) via TCP                                      |
| **Billing headers**   | Proxy **overwrites** `x-litellm-end-user-id` | OpenClaw **injects** per-session via `outboundHeaders`; proxy **passes through**      |
| **Session isolation** | Container boundary (invariant 19)            | Session key: `agent:main:{billingAccountId}:{runId}`                                  |

## Context

The sandbox runtime ([SANDBOXED_AGENTS.md](../SANDBOXED_AGENTS.md), invariants 1–12) provides isolated container execution for AI agents. OpenClaw is a third-party agent runtime that supports autonomous multi-turn tool-use loops. This spec defines how OpenClaw integrates with Cogni across both execution modes: what is disabled, how LLM calls route through the proxy, container images, invocation protocols, and billing.

## Goal

Define the invariants and design contracts for running OpenClaw in Cogni: which OpenClaw features are disabled, how the two execution modes work, how LLM calls transit the proxy, how billing is tracked through LiteLLM.

## Non-Goals

- OpenClaw Lit-based Control UI / dashboard (invariant 18)
- Passing API keys or git credentials into containers (SECRETS_HOST_ONLY)
- Multi-stage Docker image optimization (see [ini.sandboxed-agents](../../work/initiatives/ini.sandboxed-agents.md))
- Graph provider wiring, agent catalog, or bootstrap integration (see [openclaw-sandbox-controls](openclaw-sandbox-controls.md))
- Streaming passthrough from OpenClaw to client (deferred)

## Core Invariants

> Numbering continues from [SANDBOXED_AGENTS.md](../SANDBOXED_AGENTS.md) invariants 1–12.

### Shared (both modes)

13. **OPENCLAW_SANDBOX_OFF**: OpenClaw's built-in sandbox (`sandbox.mode`) is always `"off"`. Docker is not available inside the container; enabling OpenClaw's sandbox would hard-fail on `docker create`.

14. **OPENCLAW_CRON_DISABLED**: OpenClaw's cron scheduler (`cron.enabled`) is always `false`. Temporal is scheduler-of-record. Duplicate schedulers cause billing surprises and audit gaps.

15. **OPENCLAW_ELEVATED_DISABLED**: OpenClaw's elevated tool mode (`tools.elevated.enabled`) is always `false`. Elevated mode bypasses tool policy and would collapse the security boundary.

16. **COGNI_IS_MODEL_ROUTER**: OpenClaw's model providers are set to `mode: "replace"` with a single `cogni` provider. All built-in providers are stripped. The `baseUrl` differs by mode:
    - Ephemeral: `http://localhost:8080/v1` (socat → unix socket → per-run proxy)
    - Gateway: `http://llm-proxy-openclaw:8080/v1` (Docker DNS → shared proxy)

17. **NO_OPENCLAW_DASHBOARD**: OpenClaw's Control UI and bridge port are not exposed. The proxy only serves `/v1/*` (LLM) and `/health`.

18. **LITELLM_IS_BILLING_TRUTH**: OpenClaw's self-reported usage is a UX hint only. Authoritative billing comes from LiteLLM spend logs, correlated by proxy audit log.

### Ephemeral mode only

19. **EPHEMERAL_NO_GATEWAY**: The gateway service is never started inside the `network=none` sandbox. Gateway mode requires network listeners and persistent state that conflict with one-shot container semantics.

20. **ONE_RUN_ONE_SESSION**: Each `SandboxRunnerAdapter.runOnce()` invocation is a fresh OpenClaw session. Session state does not persist across runs. Conversation history is injected via workspace files (see [Agent I/O Protocol](#agent-io-protocol)).

### Gateway mode only

21. **GATEWAY_ON_INTERNAL_ONLY**: The OpenClaw gateway runs on `sandbox-internal` network only, never exposed to the public internet. Auth via token (`gateway.auth.mode: "token"`).

22. **OUTBOUND_HEADERS_PER_SESSION**: Billing headers (`x-litellm-end-user-id`, `x-litellm-spend-logs-metadata`, `x-cogni-run-id`) are set per-session via the `outboundHeaders` field on the WS `agent` call. The gateway proxy passes these through to LiteLLM without overwriting.

23. **SESSION_KEY_ISOLATION**: Each Cogni run gets a unique session key (`agent:main:{billingAccountId}:{runId}`). Concurrent users are isolated by session — no shared session state, no cross-contamination of billing headers. Validated empirically (see [research](../research/openclaw-gateway-header-injection.md#5-patch-validation-results-2026-02-09)).

---

## Design

### Architecture

#### Ephemeral Mode

```
SandboxGraphProvider
  1. Create tmp workspace, write .cogni/prompt.txt + openclaw.json
  2. SandboxRunnerAdapter.runOnce({ image: openclaw, llmProxy: ... })
  3. Parse stdout JSON → emit AiEvents
  4. Reconcile billing via proxy audit log
         │
         ▼
┌─────────────────────────────────────────────────────────────┐
│ SANDBOX CONTAINER: cogni-sandbox-openclaw (network=none)    │
│                                                             │
│  node /app/dist/index.js agent --local --agent main         │
│       --message "..." --session-id ${RUN_ID} --json         │
│                                                             │
│  Pi agent runtime (multi-turn tool loop):                   │
│    → POST localhost:8080/v1/chat/completions                │
│    ← tool_calls → execute locally → repeat                  │
│                                                             │
│  socat (localhost:8080 ↔ /llm-sock/llm.sock)               │
└────────────────────────┼────────────────────────────────────┘
                         │ unix socket (Docker volume)
                         ▼
┌─────────────────────────────────────────────────────────────┐
│ PER-RUN PROXY: nginx:alpine (sandbox-internal)              │
│  OVERWRITES: Authorization, x-litellm-end-user-id,         │
│              x-litellm-spend-logs-metadata                  │
│  Forwards → http://litellm:4000                             │
└─────────────────────────────────────────────────────────────┘
```

#### Gateway Mode

```
SandboxGraphProvider
  1. WS connect to openclaw-gateway:18789 (token auth, protocol v3)
  2. WS agent call with outboundHeaders (billing + x-cogni-run-id)
  3. Parse WS response → emit AiEvents
  4. grep proxy audit log by x-cogni-run-id for billing
         │
         ▼  WS (custom frame protocol, NOT JSON-RPC)
┌─────────────────────────────────────────────────────────────┐
│ GATEWAY: openclaw-outbound-headers (sandbox-internal:18789) │
│  Long-running, concurrent sessions                          │
│  Session key: agent:main:{billingAccountId}:{runId}         │
│                                                             │
│  Pi agent runtime per session (multi-turn tool loop):       │
│    → POST llm-proxy-openclaw:8080/v1/chat/completions      │
│      with outboundHeaders merged into outbound request      │
│    ← tool_calls → execute locally → repeat                  │
└────────────────────────┼────────────────────────────────────┘
                         │ TCP (Docker DNS)
                         ▼
┌─────────────────────────────────────────────────────────────┐
│ SHARED PROXY: llm-proxy-openclaw (sandbox-internal:8080)    │
│  INJECTS: Authorization: Bearer ${LITELLM_MASTER_KEY}       │
│  PASSES THROUGH: x-litellm-end-user-id,                    │
│    x-litellm-spend-logs-metadata, x-cogni-run-id           │
│  Forwards → http://litellm:4000                             │
│  Audit log keyed by x-cogni-run-id for per-run billing      │
└─────────────────────────────────────────────────────────────┘
```

**Key insight**: OpenClaw may make **many** LLM calls per run (one per tool-use turn). Every call transits the proxy. In ephemeral mode, the proxy injects billing headers. In gateway mode, OpenClaw injects them via per-session `outboundHeaders` and the proxy passes them through. LiteLLM tracks all calls; we never count tokens ourselves.

---

### LLM API Protocol

OpenClaw speaks standard **OpenAI Chat Completions** (`/v1/chat/completions`) via its `api: "openai-completions"` provider mode.

#### Request Flow (per LLM turn)

```
OpenClaw runtime
  │  POST http://localhost:8080/v1/chat/completions
  │  Headers: Authorization: Bearer proxy-handles-auth  (ignored by proxy)
  │           Content-Type: application/json
  │  Body: {
  │    "model": "gemini-2.5-flash",        ← LiteLLM alias, from openclaw.json
  │    "messages": [...],                   ← OpenClaw manages conversation
  │    "tools": [...],                      ← OpenClaw's tool definitions
  │    "stream": true                       ← OpenClaw streams by default
  │  }
  ▼
socat (localhost:8080 → /llm-sock/llm.sock)
  ▼
nginx proxy (overwrites all auth/billing headers)
  │  Authorization: Bearer ${LITELLM_MASTER_KEY}
  │  x-litellm-end-user-id: ${billingAccountId}
  │  x-litellm-spend-logs-metadata: {"run_id":"...","attempt":0,"graph_id":"sandbox:agent",...}
  ▼
LiteLLM (litellm:4000)
  │  Resolves model alias → OpenRouter upstream
  ▼
OpenRouter → upstream provider (Anthropic, Google, OpenAI, etc.)
```

#### What OpenClaw Sends

OpenClaw's `openai-completions` API mode sends standard requests:

- `POST /v1/chat/completions` with `stream: true`
- Tool definitions in the `tools` array (OpenAI function-calling format)
- Conversation history managed by OpenClaw's Pi runtime
- Model name matches the `id` field from the `cogni` provider's `models` array

#### What Our Proxy Does

Per the existing nginx template (unchanged from P0.5):

- **Overwrites** `Authorization` header with `Bearer ${LITELLM_MASTER_KEY}`
- **Overwrites** `x-litellm-end-user-id` with `${billingAccountId}`
- **Overwrites** `x-litellm-spend-logs-metadata` with run correlation JSON
- **Forwards** to `http://litellm:4000` (Docker DNS)
- **Streams** SSE responses back (`proxy_buffering off`, `proxy_read_timeout 300s`)
- **Logs** each request (timestamp, model, status, latency — no prompts)

#### What OpenClaw Receives

Standard OpenAI SSE streaming response. OpenClaw's Pi runtime parses `tool_calls` from the response, executes tools locally inside the container, appends tool results to the conversation, and calls the LLM again. This loop repeats until the LLM returns a text-only response (no more tool calls).

#### Routes Used

| Route                       | Used by OpenClaw                 | Purpose                                                    |
| --------------------------- | -------------------------------- | ---------------------------------------------------------- |
| `POST /v1/chat/completions` | Yes                              | All LLM calls                                              |
| `GET /v1/models`            | Possibly (model discovery)       | Proxy forwards all methods on `/v1/`; LiteLLM handles this |
| `GET /health`               | No (socat bridges, not OpenClaw) | Proxy health check                                         |
| `POST /v1/responses`        | No                               | Would require `api: "openai-responses"`                    |

---

### Container Images

#### Ephemeral: `cogni-sandbox-openclaw:latest`

Thin layer over `openclaw:local` — adds socat and the sandbox entrypoint:

```dockerfile
FROM openclaw:local
USER root
RUN apt-get update && apt-get install -y --no-install-recommends socat \
  && rm -rf /var/lib/apt/lists/*
COPY entrypoint.sh /usr/local/bin/sandbox-entrypoint.sh
RUN chmod 755 /usr/local/bin/sandbox-entrypoint.sh
ENTRYPOINT ["/usr/local/bin/sandbox-entrypoint.sh"]
```

No multi-stage build, no file pruning, no user creation (adapter sets `User` at container create time). Build: `docker build -t cogni-sandbox-openclaw services/sandbox-openclaw`.

#### Gateway: `openclaw-outbound-headers:latest`

Patched OpenClaw image with `outboundHeaders` support on the WS protocol. Published to `ghcr.io/cogni-dao/openclaw-outbound-headers:v0`. Based on `openclaw:local` (v2026.2.4). No socat needed — gateway uses TCP on `sandbox-internal` directly.

Entrypoint: `node /app/dist/index.js gateway` (port 18789, `gateway.auth.mode: "token"`).

Config bind-mounted at `/etc/openclaw/openclaw.json:ro`. State at `/tmp/openclaw-state` (writable tmpfs). See [gateway integration handoff](../research/openclaw-gateway-integration-handoff.md) for WS protocol details.

#### Entrypoint

Reuses the existing `sandbox-entrypoint.sh` pattern from P0.5 (starts socat, then execs the command). No changes needed — the entrypoint already bridges `localhost:8080 → /llm-sock/llm.sock` and runs `bash -lc "$@"`.

#### ReadonlyRootfs Compatibility

The `SandboxRunnerAdapter` sets `ReadonlyRootfs: true` with tmpfs at `/tmp` (64m) and `/run` (8m). OpenClaw needs writable paths controlled via env:

- **`/workspace/.openclaw/`** — config file (`OPENCLAW_CONFIG_PATH`) → on workspace bind mount (rw), OK
- **`/workspace/.openclaw-state/`** — sessions, transcripts, caches (`OPENCLAW_STATE_DIR`) → on workspace bind mount (rw), OK
- **`/workspace/`** — `HOME=/workspace` ensures any `$HOME` writes go to writable mount, not readonly rootfs

**Resolved (OQ-8)**: OpenClaw's `resolveStateDir()` defaults to `$HOME/.openclaw` if `OPENCLAW_STATE_DIR` is not set. Setting `OPENCLAW_STATE_DIR=/workspace/.openclaw-state` and `HOME=/workspace` prevents all writes to readonly paths. Verified in `src/config/paths.ts:49-74`.

---

### OpenClaw Configuration

The `SandboxGraphProvider` generates this config and writes it to `/workspace/.openclaw/openclaw.json` before each run.

#### Base Config (invariant across all runs)

```json5
{
  // ── Agent defaults ──
  agents: {
    defaults: {
      model: {
        primary: "cogni/${MODEL_ID}", // set per-run from GraphRunRequest
      },
      workspace: "/workspace", // container workspace root
      sandbox: { mode: "off" }, // invariant 13: OPENCLAW_SANDBOX_OFF
      skipBootstrap: true, // we pre-create workspace files
      timeoutSeconds: 540, // < container timeout (600s) for clean exit
    },
    list: [
      {
        id: "main",
        default: true,
        workspace: "/workspace",
      },
    ],
  },

  // ── Model routing ──
  models: {
    mode: "replace", // invariant 16: strip all built-in providers
    providers: {
      cogni: {
        baseUrl: "http://localhost:8080/v1", // socat → socket → proxy → LiteLLM
        apiKey: "proxy-handles-auth", // overwritten by proxy, value is irrelevant
        api: "openai-completions",
        models: [
          // ── generated at runtime from LiteLLM config ──
          // see "Model Catalog Sync" section
        ],
      },
    },
  },

  // ── Tool policy ──
  tools: {
    elevated: { enabled: false }, // invariant 15: OPENCLAW_ELEVATED_DISABLED
    deny: [
      "group:web", // web_fetch, web_search — no network
      "browser", // Chrome CDP — no network, no display
      "cron", // invariant 14: OPENCLAW_CRON_DISABLED
      "gateway", // no gateway running
      "nodes", // no device mesh
      "sessions_send", // no multi-session routing
      "sessions_spawn", // no sub-agent spawning
      "message", // no channel delivery
    ],
  },

  // ── Cron ──
  cron: {
    enabled: false, // invariant 14: OPENCLAW_CRON_DISABLED
  },

  // ── Gateway ──
  gateway: {
    mode: "local", // no gateway service
  },
}
```

#### Allowed Tools (what OpenClaw CAN do inside the sandbox)

| Tool               | Works? | Notes                                                                     |
| ------------------ | ------ | ------------------------------------------------------------------------- |
| `exec` (bash)      | Yes    | Runs commands inside the container. This is intentional.                  |
| `read`             | Yes    | Reads files in `/workspace`                                               |
| `write`            | Yes    | Writes files in `/workspace`                                              |
| `edit`             | Yes    | Edits files in `/workspace`                                               |
| `apply_patch`      | Yes    | Applies patches in `/workspace`                                           |
| `image`            | Maybe  | Uses LLM for generation — would work if model supports it, but not tested |
| `web_fetch`        | No     | Denied — `network=none`                                                   |
| `web_search`       | No     | Denied — `network=none`                                                   |
| `browser`          | No     | Denied — no Chromium, no display, no network                              |
| `cron`             | No     | Denied — Temporal is scheduler                                            |
| `memory`           | Yes    | Vector memory within workspace (local files only)                         |
| `sessions_list`    | Yes    | Read-only session introspection                                           |
| `sessions_history` | Yes    | Read-only session history                                                 |

#### Model Catalog Sync

The `cogni` provider's `models` array must list every LiteLLM model alias the agent might use. The `SandboxGraphProvider` generates this from our LiteLLM config.

**Mapping from LiteLLM config** (`platform/infra/services/runtime/configs/litellm.config.yaml`):

```json5
// Each LiteLLM model_name becomes an OpenClaw model entry:
{
  id: "gemini-2.5-flash", // matches litellm model_name
  name: "Gemini 2.5 Flash", // display name
  reasoning: false,
  input: ["text"],
  cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
  contextWindow: 200000, // approximate, from provider docs
  maxTokens: 8192,
}
```

We set all costs to `0` because we do NOT want OpenClaw tracking costs — LiteLLM is the billing truth (invariant 11).

---

### Agent I/O Protocol

#### Input

The `SandboxGraphProvider` prepares the workspace before invoking `runOnce()`:

```
/workspace/
├── .openclaw/
│   └── openclaw.json               # generated config (see above)
├── .cogni/
│   └── prompt.txt                   # user's message text (extracted from GraphRunRequest)
└── repo/                            # optional: host-cloned target repository
    └── ...
```

**`prompt.txt`**: The last user message from `GraphRunRequest.messages`, extracted as plain text. OpenClaw does not accept structured multi-message input via `--message` — it takes a single text string.

#### Invocation

The `SandboxGraphProvider` calls `runOnce()` with:

```typescript
// Environment via llmProxy.env (injected by SandboxRunnerAdapter as container Env)
const env = {
  HOME: "/workspace",
  OPENCLAW_CONFIG_PATH: "/workspace/.openclaw/openclaw.json",
  OPENCLAW_STATE_DIR: "/workspace/.openclaw-state",
  OPENCLAW_LOAD_SHELL_ENV: "0",
};

// Command via argv (adapter joins with space, entrypoint runs via bash -lc)
const argv = [
  "node /app/dist/index.js agent",
  "--local --agent main",
  `--session-id ${runId}`,
  '--message "$(cat /workspace/.cogni/prompt.txt)"',
  "--json --timeout 540",
];
// Note: adapter joins argv with " " into a single Cmd string, then
// entrypoint.sh runs it via `bash -lc "$@"` — shell expansion happens
// inside the container, so $(cat ...) resolves correctly.
```

**Effective command inside container:**

```bash
HOME=/workspace \
OPENCLAW_CONFIG_PATH=/workspace/.openclaw/openclaw.json \
OPENCLAW_STATE_DIR=/workspace/.openclaw-state \
OPENCLAW_LOAD_SHELL_ENV=0 \
node /app/dist/index.js agent \
  --local --agent main \
  --session-id "${RUN_ID}" \
  --message "$(cat /workspace/.cogni/prompt.txt)" \
  --json --timeout 540
```

**Flag breakdown**:

| Flag           | Value       | Why                                                                          |
| -------------- | ----------- | ---------------------------------------------------------------------------- |
| `--local`      | (boolean)   | Bypass gateway, run embedded agent directly                                  |
| `--agent main` | `"main"`    | Route to the default agent defined in config                                 |
| `--session-id` | `${RUN_ID}` | Unique per sandbox invocation, used for session state                        |
| `--message`    | prompt text | The user's message to the agent                                              |
| `--json`       | (boolean)   | Output structured JSON to stdout (no other output)                           |
| `--timeout`    | `540`       | Agent timeout in seconds. Must be < container `maxRuntimeSec` for clean exit |

#### Output

With `--json`, OpenClaw writes a JSON envelope to **stdout**:

```json5
{
  payloads: [
    {
      text: "The agent's final text response",
      mediaUrl: null,
    },
  ],
  meta: {
    durationMs: 12450,
    agentMeta: {
      sessionId: "run-abc123",
      provider: "cogni", // our provider name
      model: "cogni/gemini-2.5-flash",
      usage: {
        // OpenClaw-reported (UX hint only)
        input: 3200,
        output: 890,
        total: 4090,
      },
    },
    error: null, // or { "kind": "context_overflow", "message": "..." }
  },
}
```

**`SandboxGraphProvider` parsing**:

1. Parse stdout as JSON
2. Extract `payloads[0].text` → emit as `text_delta` AiEvent
3. Check `meta.error` → if present, return `GraphFinal.error`
4. `meta.agentMeta.usage` is a **UX hint only** — authoritative billing from LiteLLM `/spend/logs`
5. If stdout is not valid JSON (agent crash), treat as `GraphFinal.error` with stderr as diagnostics

#### Exit Codes

| Code    | Meaning                                         | Action                                |
| ------- | ----------------------------------------------- | ------------------------------------- |
| `0`     | Success (even if `meta.error` is set)           | Parse JSON, check `meta.error`        |
| `1`     | CLI error (missing flags, config parse failure) | Return `GraphFinal.error` with stderr |
| `137`   | OOM killed (SIGKILL)                            | Return `errorCode: "oom_killed"`      |
| timeout | Container killed by `SandboxRunnerAdapter`      | Return `errorCode: "timeout"`         |

---

### Agent Provisioning Guide

#### How OpenClaw Agents Work

Each OpenClaw agent is defined by:

1. **Config entry** in `openclaw.json` under `agents.list[]`
2. **Workspace directory** containing behavior files
3. **Model assignment** (which LLM to use)

#### Pre-Configured Agent Setup (No Interactive Onboard)

OpenClaw does NOT require `openclaw onboard` to run. It gracefully defaults to `{}` if the config file is empty. Our `SandboxGraphProvider` generates the full config, so no onboarding is needed.

**Minimum required for agent to run:**

1. `openclaw.json` exists with provider config (generated by us)
2. At least one of `--agent`, `--session-id`, or `--to` is passed (we use `--agent main`)
3. LLM endpoint is reachable (our proxy at `localhost:8080`)

#### Workspace Behavior Files

OpenClaw reads these files from the workspace directory at session start. They form the agent's system prompt:

| File           | Purpose                          | Required?                        |
| -------------- | -------------------------------- | -------------------------------- |
| `AGENTS.md`    | Main operating instructions      | Recommended                      |
| `SOUL.md`      | Agent personality and philosophy | Recommended                      |
| `USER.md`      | Context about the user/DAO       | Optional                         |
| `TOOLS.md`     | Environment-specific tool notes  | Optional                         |
| `IDENTITY.md`  | Name, emoji, avatar              | Optional                         |
| `HEARTBEAT.md` | Proactive task checklist         | Not used (cron disabled)         |
| `BOOTSTRAP.md` | First-run onboarding             | Not used (`skipBootstrap: true`) |
| `MEMORY.md`    | Long-term curated memory         | Optional                         |

**For Cogni sandbox agents**, the `SandboxGraphProvider` should pre-create at minimum:

```markdown
# /workspace/AGENTS.md

You are a sandboxed coding agent. You have access to bash, file read/write/edit.
You do NOT have internet access. All LLM calls route through localhost:8080.
Do not attempt web searches, fetches, or browser actions — they will fail.
Focus on the task in the workspace.
```

```markdown
# /workspace/SOUL.md

You are a focused, technical coding agent. Be direct and concise.
Write clean code. Explain your reasoning. Ask clarifying questions
if the task is ambiguous.
```

#### Skills

OpenClaw skills (in `skills/` directory) provide domain-specific behavior. Skills that require network access (web search, APIs) are non-functional in the sandbox. Skills that work with local files (coding patterns, documentation) work fine.

---

### Billing & Observability

#### Billing Flow

Unchanged from [SANDBOXED_AGENTS.md](../SANDBOXED_AGENTS.md) P0.5 design and [external-executor-billing](external-executor-billing.md):

1. Every LLM call from OpenClaw transits our nginx proxy
2. Proxy injects `x-litellm-end-user-id: ${billingAccountId}`
3. Proxy injects `x-litellm-spend-logs-metadata: { "run_id": "${runId}", ... }`
4. LiteLLM records spend per `end_user` with `metadata.run_id`
5. After sandbox exits, `reconcileRun()` queries `GET /spend/logs?end_user=${billingAccountId}` and filters by `metadata.run_id`
6. Emits `usage_report` AiEvent → `RunEventRelay` commits `charge_receipt`

**OpenClaw-specific consideration**: A single sandbox run may generate 5–50+ LLM calls (one per tool-use turn). All are tracked under the same `billingAccountId` + `run_id`. The reconciliation query returns all of them. Sum `completion_tokens` + `prompt_tokens` across all calls for total run cost.

#### Observability

- **Proxy audit log**: Every LLM request logged with timestamp, model, status, latency
- **LiteLLM → Langfuse**: All calls traced via `x-litellm-spend-logs-metadata` (contains `existing_trace_id`, `session_id`)
- **OpenClaw stdout**: Agent's final response and metadata (parsed by `SandboxGraphProvider`)
- **OpenClaw stderr**: Diagnostic output if agent fails (captured by `SandboxRunnerAdapter`)

#### Token Consumption Risk

OpenClaw's autonomous tool-use loop has no explicit turn limit. A pathological agent could loop indefinitely, consuming tokens until the container timeout (default 600s).

**Mitigations (layered)**:

1. Container `maxRuntimeSec` (hard kill at timeout)
2. OpenClaw `--timeout` flag (clean exit before container kill)
3. LiteLLM per-key spend limits (backstop for runaway billing)
4. Proxy `proxy_read_timeout 300s` (individual request timeout)

---

### Conversation Continuity

Each sandbox invocation creates a new OpenClaw session (invariant 19: ONE_RUN_ONE_SESSION). No conversation history persists. The user's message is passed via `--message`, and the agent responds without prior context.

This is acceptable for single-turn task execution ("fix this bug", "write this function").

---

### Anti-Patterns

| Pattern                                  | Problem                                                          |
| ---------------------------------------- | ---------------------------------------------------------------- |
| Enable OpenClaw sandbox (`mode: "all"`)  | Docker not available in container; hard crash                    |
| Enable OpenClaw cron                     | Duplicate scheduler; billing surprises                           |
| Enable elevated tools                    | Collapses sandbox security boundary                              |
| Use `models.mode: "merge"`               | Built-in providers cause auth resolution noise, confusing errors |
| Start OpenClaw gateway in sandbox        | Requires network listeners; violates one-shot semantics          |
| Pass API keys via env to sandbox         | Violates SECRETS_HOST_ONLY; proxy handles auth                   |
| Trust OpenClaw's usage reporting         | UX hint only; LiteLLM is billing truth                           |
| Run `openclaw onboard` in container      | Interactive; not needed with pre-created config                  |
| Mount Docker socket into sandbox         | Allows container escape; violates NETWORK_DEFAULT_DENY           |
| Leave `web_fetch`/`browser` enabled      | Immediate failure (`network=none`); confuses agent               |
| Proxy OpenClaw dashboard                 | Network + ports required; not until P2+                          |
| Persist sessions without workspace mount | Session state lost on container exit                             |

## Acceptance Checks

**Automated:**

- `pnpm check:docs` — validates spec frontmatter and required headings

**Manual:**

1. Verify `docker run --rm cogni-sandbox-openclaw node /app/dist/index.js --version` outputs expected version
2. Verify sandbox container has no network access (`curl` fails) and no `LITELLM_MASTER_KEY` in env
3. Verify OpenClaw JSON output parses correctly (see `tests/_fixtures/sandbox/openclaw-expected-output.json`)

## Open Questions

### OQ-2: `/v1/models` Discovery

Does OpenClaw call `GET /v1/models` at startup or during model resolution? Our nginx proxy forwards **all methods** on `/v1/` (not just POST), so `GET /v1/models` would pass through to LiteLLM, which supports this endpoint. The question is whether OpenClaw actually makes this call when models are pre-configured in `openclaw.json`, or whether it's only used for auto-discovery of available models.

**Resolution**: Likely a non-issue since the proxy forwards it and LiteLLM handles it. Verify by checking proxy access logs during an OpenClaw test run.

### OQ-3: OpenAI Completions vs Responses API

OpenClaw supports both `openai-completions` and `openai-responses`. We default to `openai-completions` because LiteLLM's OpenAI-compatible proxy speaks this format. But `openai-responses` may have benefits (reasoning separation). Does LiteLLM support the Responses API format?

**Resolution**: Use `openai-completions` for P0.75. Evaluate Responses API if LiteLLM adds support.

### OQ-5: Workspace File Ownership

The `SandboxRunnerAdapter` hardcodes `User: "sandboxer"` (uid 1001). The host-mounted workspace is created by the Node.js host process (likely running as a different uid). Will file permissions conflict?

**Resolution**: The adapter already handles this for the existing P0.5 sandbox — the same pattern applies. Create workspace with `0o777` or `chown 1001:1001` on the host before mounting. OpenClaw writes config/session files to `OPENCLAW_CONFIG_PATH` and `OPENCLAW_STATE_DIR`, both on the workspace mount.

### OQ-7: `--message` Size Limits

OpenClaw receives the prompt via CLI `--message` flag. For very long prompts (e.g., pasting an entire file for review), this could hit shell argument limits (typically 128KB–2MB depending on OS). Alternative: write message to a file and use shell substitution `--message "$(cat file)"`.

**Resolution**: Our invocation already uses `$(cat /workspace/.cogni/prompt.txt)`. Verify this works for prompts up to the shell limit. For larger inputs, consider writing a wrapper script inside the container.

## Related

- **Initiative:** [ini.sandboxed-agents](../../work/initiatives/ini.sandboxed-agents.md) — Roadmap: container, E2E wiring, custom agents, conversation continuity
- [Sandboxed Agents](../SANDBOXED_AGENTS.md) — Core sandbox invariants 1–12, phase definitions (pending migration)
- [OpenClaw Sandbox Controls](openclaw-sandbox-controls.md) — Invariants 20-25, dynamic catalog, git relay, dashboard
- [External Executor Billing](external-executor-billing.md) — Reconciliation pattern, billing invariants
- [Sandbox Scaling](../SANDBOX_SCALING.md) — Proxy architecture, threat model (pending migration)
- [Graph Execution](graph-execution.md) — GraphExecutorPort, AggregatingGraphExecutor
- [Tool Use](tool-use.md) — Tool execution, DENY_BY_DEFAULT
- [RBAC](rbac.md) — ExecutionGrant, allowSandbox gate
- OpenClaw repo: `/Users/derek/dev/openclaw/` — source of truth for OpenClaw internals
