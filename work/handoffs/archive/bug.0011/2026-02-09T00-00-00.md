---
id: openclaw-streaming-truncation-handoff
type: research
title: "Handoff: OpenClaw Gateway Streaming — Truncation + Mock-LLM Bugs"
status: active
trust: reviewed
verified: 2026-02-09
summary: Two open streaming bugs in the OpenClaw gateway integration. Full diagnostic trail, proven root causes, and next steps for a new developer.
read_when: Picking up OpenClaw gateway streaming work on feat/concurrent-openclaw branch
owner: derekg1729
created: 2026-02-09
tags: [sandbox, openclaw, gateway, streaming, handoff]
---

# Handoff: OpenClaw Gateway Streaming — Truncation + Mock-LLM Bugs

**Branch:** `feat/concurrent-openclaw`
**Date:** 2026-02-09
**Image:** `openclaw-outbound-headers:latest` (v2026.2.6-3, built from `~/dev/openclaw/`)

## Project Goal

Get OpenClaw's gateway agent fully working in the Cogni chat UI: real LLM content streaming to the user with correct billing. The gateway protocol client, billing pipeline, and proxy chain are all done. The remaining blockers are two streaming bugs.

## What's Working

- **Full proxy chain:** nginx → LiteLLM → OpenRouter → real models (nemotron-nano-30b) — content returns via `curl`
- **Gateway WS protocol client:** 3-step handshake, agent calls, typed event generator (`openclaw-gateway-client.ts`)
- **Billing:** proxy audit log → `ProxyBillingReader` → `usage_report` AiEvents → `charge_receipts` DB
- **SandboxGraphProvider:** routes `sandbox:openclaw` graphId through gateway, bridges WS events to AiEvent stream
- **Secrets isolation:** LITELLM_MASTER_KEY stays in proxy, never enters gateway container

## Open Bugs

### Bug 1: Streaming truncation in UI (bug.0011) — ACTIVE BLOCKER

**Symptom:** Real model responses are truncated mid-sentence in the Cogni chat UI. Examples:

- "...every balcony, rooftop, and community" (rest missing)
- "Sil" (only 3 characters of a haiku)

**What's proven:**

- `OPENCLAW_RAW_STREAM` log inside the container shows **full content** (text_end + assistant_message_end with complete rawText)
- `curl` against the gateway HTTP endpoint returns **full content**
- The truncation happens in **our code**, between the gateway WS events and the browser

**What's been tried (didn't fix it):**

- Added `proxy_request_buffering off` + increased `proxy_read_timeout` to nginx config — not the cause (OPENCLAW_RAW_STREAM proves content exits OpenClaw fully)
- Fixed WS close race in gateway client (replaced `ws.on("close")` → `push({done})` with a `ws_closed` sentinel + `terminalSeen` flag) — didn't fix it either

**Where to investigate next:**
The text drops somewhere in this chain:

```
OpenClaw WS frames
  → openclaw-gateway-client.ts runAgent() yields GatewayAgentEvent
    → sandbox-graph.provider.ts createGatewayExecution() yields AiEvent
      → [chat route SSE transport] writes to HTTP response
        → browser reads SSE → React state update
```

Key question: **are all text_delta events being yielded by the generator, or is the consumer (SSE transport) closing the stream early?** Instrument `createGatewayExecution()` to log every yielded event and compare against OPENCLAW_RAW_STREAM.

The gateway client uses diff-based delta streaming (lines 276-289): each WS `chat` delta carries the full accumulated text, client diffs against `prevText`. If the agent does multi-turn LLM calls (nemotron does chain-of-thought → 4 billing entries per message), the accumulated text may reset between turns, triggering the regression guard at line 279 that resets `prevText=""`. This could cause text_delta events to re-emit or skip content at turn boundaries.

**Stack test consideration:** The existing stack test uses `mock-llm` which produces one fast response. A truncation test needs a real model producing multi-token streaming output. Either use a real model in the test (slower, needs API key) or create a mock that streams slowly.

### Bug 2: Mock-LLM SSE incompatibility (bug.0009) — TESTING BLOCKER

**Symptom:** When the gateway config points at `test-model` (mock-llm backend), the agent returns `content:[]` with zero usage. HTTP endpoint returns `"No response from OpenClaw."`.

**Root cause (proven):** Mock-llm's SSE streaming format is incompatible with OpenClaw's embedded pi-ai parser. The `for await` loop over the OpenAI SDK stream produces zero iterations. Non-streaming error responses (400s) DO come through as content — only streaming fails.

**Diagnostic:** `OPENCLAW_RAW_STREAM=1` env var (already in compose) logs raw events. With mock-llm: 1 event (`assistant_message_end`, `rawText:""`). With nemotron: 38 text_delta events + full rawText.

**Options:** Fix mock-llm SSE format, use a real (free) model for gateway tests, or skip content assertions for mock-llm gateway tests.

### Bug 3: Model selection ignored (task.0010) — KNOWN LIMITATION

The gateway HTTP `/v1/chat/completions` endpoint ignores the `model` field from the request body. It always uses the agent's default model from `openclaw-gateway.json`. Config currently has `"primary": "cogni/nemotron-nano-30b"`.

## Key Files

| File                                                                | Purpose                                                                                                                                                        |
| ------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `src/adapters/server/sandbox/openclaw-gateway-client.ts`            | WS protocol client. `runAgent()` AsyncGenerator yields `GatewayAgentEvent`. Push→pull bridge with queue. **Has uncommitted WS close fix.**                     |
| `src/adapters/server/sandbox/sandbox-graph.provider.ts`             | `createGatewayExecution()` bridges WS events → AiEvent stream. Lines 426-442: for-await loop over `runAgent()`, yields `text_delta` and captures `chat_final`. |
| `src/adapters/server/sandbox/proxy-billing-reader.ts`               | Reads billing entries from nginx proxy audit log via dockerode                                                                                                 |
| `services/sandbox-openclaw/openclaw-gateway.json`                   | Gateway config: model, provider, tools, auth. Currently nemotron-nano-30b default.                                                                             |
| `platform/infra/services/sandbox-proxy/nginx-gateway.conf.template` | nginx proxy config between OpenClaw and LiteLLM. **Has uncommitted `proxy_request_buffering off` fix.**                                                        |
| `platform/infra/services/sandbox-proxy/nginx.conf.template`         | Same for ephemeral mode proxy                                                                                                                                  |
| `tests/stack/sandbox/sandbox-openclaw.stack.test.ts`                | Stack tests: WS protocol, billing, secrets isolation. Uses mock-llm (won't pass content tests).                                                                |
| `scripts/diag-openclaw-gateway.mjs`                                 | Standalone WS diagnostic script                                                                                                                                |
| `platform/infra/services/runtime/docker-compose.dev.yml`            | Compose: `openclaw-gateway` service (lines 467-505). Has `OPENCLAW_RAW_STREAM=1` env vars.                                                                     |

## Uncommitted Changes

```
nginx-gateway.conf.template  — proxy_request_buffering off + timeout 3600s
nginx.conf.template           — same
openclaw-gateway-client.ts    — WS close race fix (ws_closed sentinel + terminalSeen)
task.0010 + bug.0004           — minor doc updates
```

These changes are reasonable to keep even though they didn't fix the truncation — they're correctness improvements.

## Diagnostic Commands

```bash
# Start gateway stack
pnpm sandbox:openclaw:up

# Test via curl (should return full content with nemotron)
curl -s http://127.0.0.1:3333/v1/chat/completions \
  -H "Authorization: Bearer openclaw-internal-token" \
  -H "Content-Type: application/json" \
  -d '{"model":"cogni/nemotron-nano-30b","messages":[{"role":"user","content":"Write 3 sentences about the ocean"}]}'

# Check raw stream log (inside container)
docker exec openclaw-gateway cat /tmp/openclaw-raw-stream.jsonl

# Check gateway logs
docker logs openclaw-gateway --tail 30

# Check proxy audit log
docker exec llm-proxy-openclaw cat /tmp/audit.log

# Run stack tests (mock-llm — content tests will fail)
pnpm test:stack:dev -- tests/stack/sandbox/sandbox-openclaw.stack.test.ts
```

## Relevant Work Items

- `work/items/task.0008.gateway-client-protocol.md` — parent task (In Progress)
- `work/items/bug.0009.openclaw-v2026.2.4-empty-payloads.md` — mock-llm SSE bug
- `work/items/task.0010.openclaw-model-selection.md` — model selection
- `work/items/bug.0011.gateway-streaming-truncation.md` — streaming truncation
- `work/projects/proj.openclaw-capabilities.md` — project roadmap

## Reviewer Guidance (from prior review)

```json
{
  "streaming_fix_now": [
    "Do not push {done} from ws.on('close'). Replace with: closed=true + enqueue {kind:'ws_closed'} sentinel.",
    "Terminate generator only after terminal final-res observed AND queue drained; if ws_closed before terminal, emit error ws_closed_before_terminal and end.",
    "In provider/UI: treat deltas as UX only; always overwrite/ensure final output equals terminal result text (assistant_final)."
  ],
  "ai_event_unification_next": [
    "After truncation fix lands, introduce a minimal shared AsyncIterable<AiEvent> boundary (text_delta + assistant_final + done/error).",
    "Have each adapter map its protocol to AiEvent; keep protocol-specific parsing private so it doesn't leak everywhere."
  ],
  "not_nginx": "Given OPENCLAW_RAW_STREAM contains full text, nginx is not truncating; our termination/flush logic is."
}
```

The first item (WS close sentinel) is already implemented but didn't fix the truncation. The third item (treat deltas as UX only, overwrite with assistant_final) has NOT been implemented yet — this is likely the remaining fix needed. The `assistant_final` AiEvent is emitted with the full `chat_final` text (line 484 of `sandbox-graph.provider.ts`), but the UI/SSE transport may not use it to reconcile the displayed text.
