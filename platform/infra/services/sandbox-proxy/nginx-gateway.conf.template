# Gateway LLM Proxy - Nginx Config Template
# Long-running proxy for OpenClaw gateway mode.
# Passes through billing headers from OpenClaw (does NOT overwrite them).
# Only injects Authorization header for LiteLLM auth.
#
# Template variables (substituted at runtime via envsubst in compose):
#   ${LITELLM_MASTER_KEY}   - LiteLLM API key (injected as Authorization header)
#
# Audit log captures x-cogni-run-id for per-run correlation without JSON parsing.
# Billing headers (x-litellm-end-user-id, x-litellm-spend-logs-metadata) are
# set by OpenClaw per-session via outboundHeaders — proxy passes them through.

worker_processes 1;
error_log stderr warn;
pid /tmp/nginx-gateway.pid;

events {
    worker_connections 128;
}

http {
    # Audit log: captures run_id + end_user from client headers + cost from upstream
    log_format audit '$time_iso8601 '
                     'run_id=$http_x_cogni_run_id '
                     'end_user=$http_x_litellm_end_user_id '
                     'status=$status '
                     'litellm_call_id=$upstream_http_x_litellm_call_id '
                     'litellm_response_cost=$upstream_http_x_litellm_response_cost';

    access_log /tmp/audit.log audit;

    # Upstream: LiteLLM on sandbox-internal network
    upstream litellm {
        server litellm:4000;
        keepalive 8;
    }

    server {
        # Listen on TCP (long-running, shared by all sessions)
        listen 8080;

        # OpenAI-compatible API routes
        location /v1/ {
            # Inject authorization only (per SECRETS_HOST_ONLY)
            proxy_set_header Authorization "Bearer ${LITELLM_MASTER_KEY}";

            # Pass through billing + observability headers from OpenClaw
            # (proxy does NOT overwrite — OpenClaw sets per-session values)

            # Pass LiteLLM response headers back to client
            proxy_pass_header x-litellm-call-id;
            proxy_pass_header x-litellm-response-cost;

            # Forward to LiteLLM
            proxy_pass http://litellm;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header Connection "";

            # Streaming support (SSE for chat completions)
            proxy_buffering off;
            proxy_request_buffering off;
            proxy_cache off;
            proxy_read_timeout 3600s;

            # Pass through content type
            proxy_set_header Content-Type $content_type;
        }

        # Health check endpoint
        location /health {
            return 200 '{"status":"ok","service":"llm-proxy-openclaw"}';
            add_header Content-Type application/json;
        }

        # Deny all other paths
        location / {
            return 404 '{"error":"not_found"}';
            add_header Content-Type application/json;
        }
    }
}
