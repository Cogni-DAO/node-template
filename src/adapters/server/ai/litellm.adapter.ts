// SPDX-License-Identifier: LicenseRef-PolyForm-Shield-1.0.0
// SPDX-FileCopyrightText: 2025 Cogni-DAO

/**
 * Module: `@adapters/server/ai/litellm`
 * Purpose: LiteLLM service implementation for AI completion and streaming with cost extraction and runtime secret validation.
 * Scope: Implements LlmService port (completion + stream), extracts cost from headers, validates secrets at adapter boundary. Does not handle auth or rate-limiting.
 * Invariants: Never logs prompts/keys/chunks; 30s timeout (completion), 15s connect timeout (stream); settles once; model required; stream abort rejects with LlmError(kind='aborted').
 * Side-effects: IO (HTTP calls to LiteLLM)
 * Notes: SSE via eventsource-parser; assertRuntimeSecrets() before fetch; logs only bounded metadata (no content); aborted streams are errors, not partial successes.
 * Links: LlmService port, serverEnv, assertRuntimeSecrets, defer<T>() for promise settlement
 * @internal
 */

import {
  createParser,
  type EventSourceMessage,
  type EventSourceParser,
} from "eventsource-parser";
import {
  type ChatDeltaEvent,
  classifyLlmErrorFromStatus,
  LlmError,
  type LlmService,
} from "@/ports";
import {
  computePromptHash,
  DEFAULT_MAX_TOKENS,
  DEFAULT_TEMPERATURE,
} from "@/shared/ai/prompt-hash";
import { serverEnv } from "@/shared/env";
import { assertRuntimeSecrets } from "@/shared/env/invariants";
import { makeLogger } from "@/shared/observability";

const logger = makeLogger({ component: "LiteLlmAdapter" });

/**
 * Extract provider name from LiteLLM model ID prefix.
 * e.g., "openai/gpt-4" → "openai", "anthropic/claude-3" → "anthropic"
 * Falls back to "unknown" if no prefix.
 */
function extractProviderFromModel(model: string): string {
  const slashIndex = model.indexOf("/");
  if (slashIndex > 0) {
    return model.slice(0, slashIndex);
  }
  // Fallback: try to infer from known model prefixes
  if (model.startsWith("gpt-") || model.startsWith("o1")) return "openai";
  if (model.startsWith("claude-")) return "anthropic";
  return "unknown";
}

/**
 * Create a deferred promise with resolve/reject callbacks.
 * Ensures promise settles exactly once.
 */
function defer<T>() {
  let settled = false;
  let resolve!: (value: T) => void;
  let reject!: (reason?: unknown) => void;
  const promise = new Promise<T>((res, rej) => {
    resolve = (value) => {
      if (!settled) {
        settled = true;
        res(value);
      }
    };
    reject = (reason) => {
      if (!settled) {
        settled = true;
        rej(reason);
      }
    };
  });
  return { promise, resolve, reject };
}

/**
 * Extract provider cost from LiteLLM response headers.
 * Quarantines null at the boundary and returns number | undefined.
 */
function getProviderCostFromHeaders(response: Response): number | undefined {
  const raw = response.headers.get("x-litellm-response-cost");
  if (!raw || raw.trim().length === 0) return undefined;

  const parsed = Number.parseFloat(raw);
  return Number.isFinite(parsed) ? parsed : undefined;
}

/**
 * Extract LiteLLM call ID from response headers for forensic correlation.
 * Returns undefined if header is absent or empty.
 */
function getLitellmCallIdFromHeaders(response: Response): string | undefined {
  const raw = response.headers.get("x-litellm-call-id");
  if (!raw || raw.trim().length === 0) return undefined;
  return raw.trim();
}

export class LiteLlmAdapter implements LlmService {
  async completion(
    params: Parameters<LlmService["completion"]>[0]
  ): ReturnType<LlmService["completion"]> {
    // Model must be provided by caller (route validates via contract)
    if (!params.model) {
      throw new Error("LiteLLM completion requires model parameter");
    }
    const model = params.model;
    const temperature = params.temperature ?? DEFAULT_TEMPERATURE;
    const maxTokens = params.maxTokens ?? DEFAULT_MAX_TOKENS;

    // Extract caller data for user attribution and correlation (cost tracking in LiteLLM)
    const { billingAccountId, requestId, traceId } = params.caller;

    // Convert core Messages to LiteLLM format
    const liteLlmMessages = params.messages.map((msg) => ({
      role: msg.role,
      content: msg.content,
    }));

    // Compute prompt hash BEFORE adding metadata (per AI_SETUP_SPEC.md)
    const promptHash = computePromptHash({
      model,
      messages: liteLlmMessages,
      temperature,
      maxTokens,
    });

    const requestBody = {
      model,
      messages: liteLlmMessages,
      temperature,
      max_tokens: maxTokens,
      user: billingAccountId, // LiteLLM user tracking for cost attribution
      metadata: {
        cogni_billing_account_id: billingAccountId,
        request_id: requestId,
        trace_id: traceId,
      },
    };

    const env = serverEnv();
    // Validate runtime secrets at adapter boundary (not in serverEnv to avoid breaking Next.js build)
    assertRuntimeSecrets(env);

    let response: Response;
    try {
      // HTTP call to LiteLLM with timeout enforcement
      // Uses LITELLM_MASTER_KEY (server-only secret) - never expose per-user virtual keys
      response = await fetch(`${env.LITELLM_BASE_URL}/v1/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${env.LITELLM_MASTER_KEY}`,
        },
        body: JSON.stringify(requestBody),
        /** 30 second timeout */
        signal: AbortSignal.timeout(30000),
      });
    } catch (error) {
      // Handle fetch errors (network, timeout, abort)
      if (error instanceof Error) {
        if (error.name === "AbortError" || error.name === "TimeoutError") {
          throw new LlmError(`LiteLLM request timed out`, "timeout", 408);
        }
        throw new LlmError(
          `LiteLLM network error: ${error.message}`,
          "unknown"
        );
      }
      throw new LlmError("LiteLLM completion failed: Unknown error", "unknown");
    }

    // Handle HTTP errors with typed LlmError (per AI_SETUP_SPEC.md)
    if (!response.ok) {
      const kind = classifyLlmErrorFromStatus(response.status);
      throw new LlmError(
        `LiteLLM API error: ${response.status} ${response.statusText}`,
        kind,
        response.status
      );
    }

    // Read cost and call ID from response headers
    const providerCostFromHeader = getProviderCostFromHeaders(response);
    const litellmCallId = getLitellmCallIdFromHeaders(response);

    const data = (await response.json()) as {
      id?: string;
      model?: string;
      choices: { message: { content: string }; finish_reason?: string }[];
      usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens?: number;
      };
    };

    if (
      !data.choices ||
      data.choices.length === 0 ||
      !data.choices[0]?.message ||
      typeof data.choices[0].message.content !== "string"
    ) {
      throw new LlmError("Invalid response from LiteLLM", "unknown");
    }

    // Build result object conditionally to satisfy exactOptionalPropertyTypes
    const promptTokens = Number(data.usage?.prompt_tokens) || 0;
    const completionTokens = Number(data.usage?.completion_tokens) || 0;
    const totalTokens = data.usage?.total_tokens
      ? Number(data.usage.total_tokens)
      : promptTokens + completionTokens;

    // Extract resolved model from response (may differ from requested model)
    const resolvedModel = data.model ?? model;
    const resolvedProvider = extractProviderFromModel(resolvedModel);

    const result: Awaited<ReturnType<LlmService["completion"]>> = {
      message: {
        role: "assistant",
        content: data.choices[0].message.content,
      },
      usage: {
        promptTokens,
        completionTokens,
        totalTokens,
      },
      providerMeta: data as unknown as Record<string, unknown>,
      promptHash,
      resolvedProvider,
      resolvedModel,
    };

    // Add optional fields only when present
    if (data.choices[0].finish_reason) {
      result.finishReason = data.choices[0].finish_reason;
    }

    if (typeof providerCostFromHeader === "number") {
      result.providerCostUsd = providerCostFromHeader;
    }

    // Prefer response body id (gen-...) for join with /spend/logs
    if (data.id) {
      result.litellmCallId = data.id;
    } else if (litellmCallId) {
      result.litellmCallId = litellmCallId;
    }

    // Sanitized adapter log (no content, bounded fields only)
    // Use final resolved call ID for hasCallId (not header-only value)
    logger.info(
      {
        model: resolvedModel,
        provider: resolvedProvider,
        tokensUsed: totalTokens,
        finishReason: result.finishReason,
        hasCost: typeof providerCostFromHeader === "number",
        hasCallId: !!result.litellmCallId,
        contentLength: data.choices[0].message.content.length,
        promptHash,
      },
      "adapter.litellm.completion_result"
    );

    return result;
  }

  async completionStream(
    params: Parameters<LlmService["completionStream"]>[0]
  ): ReturnType<LlmService["completionStream"]> {
    // Model must be provided by caller (route validates via contract)
    if (!params.model) {
      throw new Error("LiteLLM completionStream requires model parameter");
    }
    const model = params.model;
    const temperature = params.temperature ?? DEFAULT_TEMPERATURE;
    const maxTokens = params.maxTokens ?? DEFAULT_MAX_TOKENS;
    const { billingAccountId, requestId, traceId } = params.caller;

    const liteLlmMessages = params.messages.map((msg) => ({
      role: msg.role,
      content: msg.content,
    }));

    // Compute prompt hash BEFORE adding metadata (per AI_SETUP_SPEC.md)
    const promptHash = computePromptHash({
      model,
      messages: liteLlmMessages,
      temperature,
      maxTokens,
    });

    const requestBody = {
      model,
      messages: liteLlmMessages,
      temperature,
      max_tokens: maxTokens,
      user: billingAccountId, // LiteLLM user tracking for cost attribution
      metadata: {
        cogni_billing_account_id: billingAccountId,
        request_id: requestId,
        trace_id: traceId,
      },
      stream: true,
      stream_options: { include_usage: true }, // Request usage in stream if supported
    };

    let response: Response;
    // Use short timeout for connection/TTFB only (not entire stream duration)
    const connectCtl = new AbortController();
    const connectTimer = setTimeout(() => connectCtl.abort(), 15000);
    const env = serverEnv();
    // Validate runtime secrets at adapter boundary
    assertRuntimeSecrets(env);

    try {
      const signal = params.abortSignal
        ? AbortSignal.any([connectCtl.signal, params.abortSignal])
        : connectCtl.signal;

      // Uses LITELLM_MASTER_KEY (server-only secret) - never expose per-user virtual keys
      response = await fetch(`${env.LITELLM_BASE_URL}/v1/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${env.LITELLM_MASTER_KEY}`,
        },
        body: JSON.stringify(requestBody),
        signal,
      });
    } catch (error) {
      // Handle fetch errors (network, timeout, abort)
      if (error instanceof Error) {
        if (error.name === "AbortError") {
          throw new LlmError("LiteLLM stream aborted", "aborted");
        }
        if (error.name === "TimeoutError") {
          throw new LlmError(
            "LiteLLM stream connection timed out",
            "timeout",
            408
          );
        }
        throw new LlmError(
          `LiteLLM stream init failed: ${error.message}`,
          "unknown"
        );
      }
      throw new LlmError(
        "LiteLLM stream init failed: Unknown error",
        "unknown"
      );
    } finally {
      clearTimeout(connectTimer);
    }

    // Handle HTTP errors with typed LlmError (per AI_SETUP_SPEC.md)
    if (!response.ok) {
      const kind = classifyLlmErrorFromStatus(response.status);
      throw new LlmError(
        `LiteLLM API error: ${response.status} ${response.statusText}`,
        kind,
        response.status
      );
    }

    // Capture response.body to prove non-null to TypeScript
    const body = response.body;
    if (!body) {
      throw new Error("LiteLLM response body is empty");
    }

    // Capture cost and call ID from headers if available immediately (unlikely for stream)
    const providerCostUsd = getProviderCostFromHeaders(response);
    const litellmCallId = getLitellmCallIdFromHeaders(response);

    // Create a deferred promise for the final result (matches completion() return type)
    type CompletionResult = Awaited<ReturnType<LlmService["completion"]>>;
    const deferred = defer<CompletionResult>();

    const stream: AsyncIterable<ChatDeltaEvent> =
      (async function* (): AsyncGenerator<ChatDeltaEvent> {
        const reader = body.getReader();
        const decoder = new TextDecoder();
        let fullContent = "";
        let finalUsage:
          | {
              promptTokens: number;
              completionTokens: number;
              totalTokens: number;
            }
          | undefined;
        let finishReason: string | undefined;
        let usageCost: number | undefined; // Cost from stream usage event
        let litellmRequestId: string | undefined; // LiteLLM's gen-... ID from response
        let streamCompleted = false; // Track if stream completed normally (not aborted/errored)

        // Queue for parsed events from eventsource-parser
        const eventQueue: EventSourceMessage[] = [];

        const parser: EventSourceParser = createParser({
          onEvent(event: EventSourceMessage) {
            eventQueue.push(event);
          },
        });

        try {
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            // Feed decoded chunk to eventsource-parser
            const chunk = decoder.decode(value, { stream: true });
            parser.feed(chunk);

            // Process all queued events
            while (eventQueue.length > 0) {
              const event = eventQueue.shift();
              if (!event) break;
              const data = event.data;

              // TODO(stream-hang-risk): streamCompleted is only set when '[DONE]' is seen.
              // If LiteLLM/provider doesn't emit '[DONE]' but ends the stream normally,
              // the `final` promise will never resolve (hang). Current OpenRouter behavior
              // always emits '[DONE]', but this is a known fragility. Options to fix:
              // 1. Track hadErrorOrAbort flag and resolve in finally when reader ends normally
              // 2. Add timeout on `final` in higher layers to convert hangs to 'timeout' errors
              // 3. Add contract test asserting '[DONE]' is always emitted
              if (data === "[DONE]") {
                streamCompleted = true;
                yield { type: "done" } as const;
                continue;
              }

              try {
                const json = JSON.parse(data);

                // Capture LiteLLM request ID (gen-...) from first chunk for join with /spend/logs
                if (json.id && !litellmRequestId) {
                  litellmRequestId = json.id;
                }

                // Check for provider error in response
                if (json.error) {
                  const errorMsg =
                    typeof json.error === "string"
                      ? json.error
                      : json.error.message || "Provider error";
                  const errorText = `LiteLLM stream error: ${errorMsg}`;
                  // Extract status code if available for proper error classification
                  const statusCode =
                    typeof json.error?.code === "number"
                      ? json.error.code
                      : undefined;
                  const errorKind = statusCode
                    ? classifyLlmErrorFromStatus(statusCode)
                    : "unknown";
                  yield { type: "error", error: errorText } as const;
                  deferred.reject(
                    new LlmError(errorText, errorKind, statusCode)
                  );
                  return;
                }

                if (json.usage) {
                  finalUsage = {
                    promptTokens: json.usage.prompt_tokens,
                    completionTokens: json.usage.completion_tokens,
                    totalTokens: json.usage.total_tokens,
                  };
                  // Extract cost from usage event (stream_options: { include_usage: true })
                  if (
                    typeof json.usage.cost === "number" &&
                    Number.isFinite(json.usage.cost)
                  ) {
                    usageCost = json.usage.cost;
                  }
                }

                const choice = json.choices?.[0];
                if (choice) {
                  if (choice.finish_reason) {
                    finishReason = choice.finish_reason;
                  }

                  const content = choice.delta?.content;
                  if (content) {
                    fullContent += content;
                    yield { type: "text_delta", delta: content } as const;
                  }
                }
              } catch (parseError) {
                // Log malformed JSON but continue streaming (transient SSE noise)
                const errorMessage =
                  parseError instanceof Error
                    ? parseError.message
                    : "JSON parse error";
                logger.warn(
                  { dataLength: data.length },
                  `Malformed SSE data: ${errorMessage}`
                );
                // Do not yield error - continue processing remaining events
              }
            }
          }
        } catch (error: unknown) {
          if (error instanceof Error && error.name === "AbortError") {
            // Stream aborted - reject with typed LlmError for proper telemetry
            deferred.reject(new LlmError("LiteLLM stream aborted", "aborted"));
            return;
          } else {
            // Real stream failure
            deferred.reject(error);
            return;
          }
        } finally {
          reader.releaseLock();

          // Only resolve on successful stream completion (not abort/error)
          if (streamCompleted) {
            // Extract resolved model/provider (SSE doesn't return these, use request param)
            const resolvedModel = model;
            const resolvedProvider = extractProviderFromModel(resolvedModel);

            // Build result object conditionally to satisfy exactOptionalPropertyTypes
            const result: CompletionResult = {
              message: { role: "assistant", content: fullContent },
              promptHash,
              resolvedProvider,
              resolvedModel,
            };
            if (finalUsage) {
              result.usage = finalUsage;
            }
            if (finishReason) {
              result.finishReason = finishReason;
            }

            // Cost derivation priority (ACTIVITY_METRICS.md §3):
            // 1. Header (providerCostUsd from x-litellm-response-cost)
            // 2. Usage event (usageCost from stream usage.cost)
            // 3. Neither → undefined (will log CRITICAL in completion.ts)
            const derivedCost =
              typeof providerCostUsd === "number" ? providerCostUsd : usageCost;

            if (typeof derivedCost === "number") {
              result.providerCostUsd = derivedCost;
            }

            // Prefer response body id (gen-...) for join with /spend/logs
            // Fall back to header UUID if response id not available
            if (litellmRequestId) {
              result.litellmCallId = litellmRequestId;
            } else if (litellmCallId) {
              result.litellmCallId = litellmCallId;
            }

            // ALWAYS include providerMeta with model (SSE doesn't return this, use request param)
            result.providerMeta = {
              model: resolvedModel,
              provider: resolvedProvider,
            };

            // Sanitized adapter log (no content, bounded fields only)
            // Use final resolved call ID for hasCallId (not header-only value)
            logger.info(
              {
                model: resolvedModel,
                provider: resolvedProvider,
                tokensUsed: finalUsage?.totalTokens,
                finishReason,
                hasCost: typeof derivedCost === "number",
                hasCallId: !!result.litellmCallId,
                contentLength: fullContent.length,
                promptHash,
              },
              "adapter.litellm.stream_result"
            );
            deferred.resolve(result);
          }
        }
      })();

    return {
      stream,
      final: deferred.promise,
    };
  }
}
