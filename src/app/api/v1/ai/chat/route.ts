// SPDX-License-Identifier: LicenseRef-PolyForm-Shield-1.0.0
// SPDX-FileCopyrightText: 2025 Cogni-DAO

/**
 * Module: `@app/api/v1/ai/chat`
 * Purpose: HTTP endpoint for chat API with assistant-ui wire format compatibility.
 * Scope: Transforms between assistant-ui wire format and DTO types for completion facade. Non-streaming in v0. Does not implement business logic or persistence.
 * Invariants: Validates with contract, transforms wire ↔ core, delegates to completion facade
 * Side-effects: IO (HTTP request/response)
 * Notes: v0 returns JSON; v1 will return SSE stream. threadId generated by client even in v0 for v2 readiness.
 * Links: Uses ai.chat.v1 contract, delegates to completion facade
 * @public
 */

import { randomUUID } from "node:crypto";
import { NextResponse } from "next/server";

import { completion } from "@/app/_facades/ai/completion.server";
import { getSessionUser } from "@/app/_lib/auth/session";
import { wrapRouteHandlerWithLogging } from "@/bootstrap/http";
import {
  aiChatOperation,
  type ChatInput,
  type ChatOutput,
} from "@/contracts/ai.chat.v1.contract";
import { isAccountsFeatureError } from "@/features/accounts/public";
import {
  aiChatStreamDurationMs,
  logRequestWarn,
  type RequestContext,
} from "@/shared/observability";

export const dynamic = "force-dynamic";
export const runtime = "nodejs";

/**
 * Internal message DTO for completion facade
 * Matches MessageDto from @features/ai/services/mappers
 */
interface MessageDto {
  role: "user" | "assistant";
  content: string;
  timestamp?: string;
}

/**
 * Transform wire format (assistant-ui) → MessageDto for completion facade
 * content: [{type: "text", text: string}] → content: string
 */
function toMessageDtos(wireMessages: ChatInput["messages"]): MessageDto[] {
  return wireMessages.map((m) => ({
    role: m.role,
    content: m.content.map((p) => p.text).join("\n"),
    timestamp: m.createdAt,
  }));
}

/**
 * Local error handler for chat route.
 * Maps domain errors to HTTP responses; returns null for unhandled errors.
 */
function handleRouteError(
  ctx: RequestContext,
  error: unknown,
  model?: string
): NextResponse | null {
  // Zod validation errors
  if (error && typeof error === "object" && "issues" in error) {
    logRequestWarn(ctx.log, error, "VALIDATION_ERROR");
    return NextResponse.json(
      { error: "Invalid input format" },
      { status: 400 }
    );
  }

  // Accounts feature errors
  if (isAccountsFeatureError(error)) {
    if (error.kind === "INSUFFICIENT_CREDITS") {
      logRequestWarn(ctx.log, error, "INSUFFICIENT_CREDITS");
      return NextResponse.json(
        { error: "Insufficient credits" },
        { status: 402 }
      );
    }
    if (error.kind === "BILLING_ACCOUNT_NOT_FOUND") {
      logRequestWarn(ctx.log, error, "BILLING_ACCOUNT_NOT_FOUND");
      return NextResponse.json({ error: "Account not found" }, { status: 403 });
    }
    if (error.kind === "VIRTUAL_KEY_NOT_FOUND") {
      logRequestWarn(ctx.log, error, "VIRTUAL_KEY_NOT_FOUND");
      return NextResponse.json(
        { error: "Virtual key not found" },
        { status: 403 }
      );
    }
    // Fallback for GENERIC
    logRequestWarn(ctx.log, error, "ACCOUNT_ERROR");
    return NextResponse.json(
      { error: error.kind === "GENERIC" ? error.message : "Account error" },
      { status: 400 }
    );
  }

  // LLM-specific errors
  if (error instanceof Error) {
    if (
      error.message.includes("MESSAGE_TOO_LONG") ||
      error.message.includes("INVALID_CONTENT")
    ) {
      logRequestWarn(ctx.log, error, "MESSAGE_VALIDATION_ERROR");
      return NextResponse.json({ error: error.message }, { status: 400 });
    }
    if (
      error.message.includes("timeout") ||
      error.message.includes("AbortError")
    ) {
      logRequestWarn(ctx.log, error, "REQUEST_TIMEOUT");
      return NextResponse.json({ error: "Request timeout" }, { status: 408 });
    }
    if (error.message.includes("LiteLLM API error: 429")) {
      logRequestWarn(ctx.log, error, "RATE_LIMIT_EXCEEDED");
      return NextResponse.json(
        { error: "Rate limit exceeded" },
        { status: 429 }
      );
    }
    if (
      error.message.includes("LiteLLM API error: 404") ||
      error.message.includes("No endpoints found")
    ) {
      logRequestWarn(ctx.log, error, "MODEL_UNAVAILABLE");
      return NextResponse.json(
        { code: "MODEL_UNAVAILABLE", model },
        { status: 409 }
      );
    }
    if (error.message.includes("LiteLLM")) {
      logRequestWarn(ctx.log, error, "LLM_SERVICE_UNAVAILABLE");
      return NextResponse.json(
        { error: "AI service temporarily unavailable" },
        { status: 503 }
      );
    }
  }

  return null; // Unhandled → let wrapper catch as 500
}

export const POST = wrapRouteHandlerWithLogging(
  { routeId: "ai.chat", auth: { mode: "required", getSessionUser } },
  async (ctx, request, sessionUser) => {
    let input: ChatInput | undefined;
    try {
      // Parse JSON body
      let body: unknown;
      try {
        body = await request.json();
      } catch {
        return NextResponse.json(
          { error: "Invalid JSON body" },
          { status: 400 }
        );
      }

      // Validate input with contract (safeParse for better error handling)
      const inputParseResult = aiChatOperation.input.safeParse(body);
      if (!inputParseResult.success) {
        logRequestWarn(ctx.log, inputParseResult.error, "VALIDATION_ERROR");
        return NextResponse.json(
          {
            error: "Invalid input",
            details: inputParseResult.error.flatten(),
          },
          { status: 400 }
        );
      }
      input = inputParseResult.data;

      // Log request received (billingAccountId will be resolved in facade, log after validation)
      const handlerStartMs = performance.now();

      // Validate model against cached allowlist (MVP-004: PERF-001 fix)
      const { isModelAllowed, getDefaults } = await import(
        "@/shared/ai/model-catalog.server"
      );
      const modelIsValid = await isModelAllowed(input.model);

      // Check for streaming request (explicit flag or Accept header)
      const accept = request.headers.get("accept") ?? "";
      const isStreaming =
        input.stream === true || accept.includes("text/event-stream");

      if (!modelIsValid) {
        // Return 409 with defaultModelId for client retry (MVP-004: UX-001 fix)
        const defaults = await getDefaults();
        logRequestWarn(
          ctx.log,
          {
            model: input.model,
            isStreaming,
            defaultModelId: defaults.defaultPreferredModelId,
          },
          "model_validation_failed"
        );
        return NextResponse.json(
          {
            error: "Invalid model",
            defaultModelId: defaults.defaultPreferredModelId,
          },
          { status: 409 }
        );
      }

      // Log request received with validated inputs
      ctx.log.info(
        {
          reqId: ctx.reqId,
          userId: sessionUser?.id,
          stream: isStreaming,
          requestedModel: input.model,
          messageCount: input.messages.length,
        },
        "ai.chat_received"
      );

      if (isStreaming) {
        const { completionStream } = await import(
          "@/app/_facades/ai/completion.server"
        );

        // Transform wire format to DTO
        const messageDtos = toMessageDtos(input.messages);

        if (!sessionUser) throw new Error("sessionUser required");

        const { stream: deltaStream, final } = await completionStream(
          {
            messages: messageDtos,
            model: input.model,
            sessionUser,
            abortSignal: request.signal,
          },
          ctx
        );

        const encoder = new TextEncoder();
        let messageId: string | undefined;
        const streamStartMs = performance.now();
        let terminalEventEmitted = false;
        let streamAborted = false;
        let abortReason: string | undefined;

        const readableStream = new ReadableStream({
          async start(controller) {
            try {
              // Generate ID once for the stream
              messageId = randomUUID();

              // Emit message.started
              controller.enqueue(
                encoder.encode(
                  `event: message.started\ndata: ${JSON.stringify({
                    messageId,
                    threadId: input?.threadId ?? randomUUID(),
                    role: "assistant",
                    createdAt: new Date().toISOString(),
                  })}\n\n`
                )
              );

              for await (const event of deltaStream) {
                if (event.type === "text_delta") {
                  controller.enqueue(
                    encoder.encode(
                      `event: message.delta\ndata: ${JSON.stringify({
                        messageId,
                        delta: event.delta,
                      })}\n\n`
                    )
                  );
                } else if (event.type === "error") {
                  controller.enqueue(
                    encoder.encode(
                      `event: error\ndata: ${JSON.stringify({
                        message: event.error,
                      })}\n\n`
                    )
                  );
                }
              }

              // Wait for final result (billing) with 15s timeout
              const finalTimeout = new Promise<{ timedOut: true }>((resolve) =>
                setTimeout(() => resolve({ timedOut: true }), 15000)
              );

              const result = await Promise.race([
                final.then(() => ({ timedOut: false })),
                finalTimeout,
              ]);

              if (result.timedOut) {
                // Deterministic terminal event: timeout (do NOT throw)
                ctx.log.warn(
                  { reqId: ctx.reqId, timeoutMs: 15000 },
                  "ai.chat_stream_finalization_lost"
                );
                terminalEventEmitted = true;
              } else {
                // Terminal event: ai.llm_call_completed emitted in completion.ts
                terminalEventEmitted = true;
              }

              // Emit message.completed
              controller.enqueue(
                encoder.encode(
                  `event: message.completed\ndata: ${JSON.stringify({
                    messageId,
                    content: "", // Content already streamed via deltas
                  })}\n\n`
                )
              );
            } catch (error) {
              if (error instanceof Error && error.name === "AbortError") {
                streamAborted = true;
                abortReason = "client_disconnect";
              } else {
                // Real stream failure
                ctx.log.error({ err: error }, "Stream error in route");
                controller.enqueue(
                  encoder.encode(
                    `event: error\ndata: ${JSON.stringify({
                      message:
                        error instanceof Error
                          ? error.message
                          : "Unknown error",
                    })}\n\n`
                  )
                );
              }
            } finally {
              controller.close();

              // Always emit stream_closed and record metrics
              const streamMs = performance.now() - streamStartMs;
              ctx.log.info(
                {
                  reqId: ctx.reqId,
                  streamMs,
                  aborted: streamAborted,
                  abortReason,
                  terminalEventEmitted,
                },
                "ai.chat_stream_closed"
              );

              // Record stream duration metric
              aiChatStreamDurationMs.observe(streamMs);
            }
          },

          cancel(reason) {
            // Client aborted - log once and stop upstream work
            streamAborted = true;
            abortReason = reason || "client_disconnect";
            ctx.log.info(
              { reqId: ctx.reqId, reason: abortReason },
              "ai.chat_client_aborted"
            );
            // AbortSignal already wired to completionStream via request.signal
          },
        });

        // Log response started before returning
        ctx.log.info(
          {
            reqId: ctx.reqId,
            handlerMs: performance.now() - handlerStartMs,
            resolvedModel: input.model,
            stream: true,
          },
          "ai.chat_response_started"
        );

        return new NextResponse(readableStream, {
          headers: {
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache, no-transform",
            Connection: "keep-alive",
          },
        });
      }

      // Transform wire → MessageDto format for facade
      const messageDtos = toMessageDtos(input.messages);

      // Delegate to completion facade with model parameter
      if (!sessionUser) throw new Error("sessionUser required"); // Enforced by wrapper
      const result = await completion(
        { messages: messageDtos, model: input.model, sessionUser },
        ctx
      );

      // Transform facade response → wire format
      // result.message is always assistant role with requestId already included
      const messageId = randomUUID();
      const wireMessage: ChatOutput["message"] = {
        id: messageId,
        role: "assistant",
        createdAt: result.message.timestamp,
        content: [{ type: "text", text: result.message.content }],
        requestId: result.message.requestId,
      };

      // Build output
      const output: ChatOutput = {
        threadId: input.threadId,
        message: wireMessage,
      };

      // Validate output with contract
      const outputParseResult = aiChatOperation.output.safeParse(output);
      if (!outputParseResult.success) {
        ctx.log.error(
          { error: outputParseResult.error },
          "Output validation failed"
        );
        return NextResponse.json(
          { error: "Internal server error" },
          { status: 500 }
        );
      }

      return NextResponse.json(outputParseResult.data);
    } catch (error) {
      const errorResponse = handleRouteError(ctx, error, input?.model);
      if (errorResponse) return errorResponse;
      throw error; // Unhandled → wrapper catches
    }
  }
);
