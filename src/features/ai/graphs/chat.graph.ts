// SPDX-License-Identifier: LicenseRef-PolyForm-Shield-1.0.0
// SPDX-FileCopyrightText: 2025 Cogni-DAO

/**
 * Module: `@features/ai/graphs/chat.graph`
 * Purpose: Chat graph definition (pure logic, no IO).
 * Scope: Orchestrates multi-step chat with optional tool use. Does not import adapters.
 * Invariants:
 *   - GRAPHS_NO_IO: No IO/adapter imports; all effects via injected ports
 *   - GRAPHS_USE_TOOLRUNNER_ONLY: Tools invoked exclusively via toolRunner.exec()
 * Side-effects: none (pure logic; effects via injected deps)
 * Notes: P1 skeleton - full LangGraph integration in future
 * Links: ai_runtime.ts, tool-runner.ts, AI_SETUP_SPEC.md
 * @internal
 */

import type { Message } from "@/core";
import type { LlmService } from "@/ports";

import type { ToolRunner } from "../tool-runner";
import type { AiEvent } from "../types";

/**
 * Graph name for telemetry.
 * Per AI_SETUP_SPEC.md: graph_name and graph_version required with graphRunId.
 * graph_version is passed at invocation time (not a module constant).
 */
export const CHAT_GRAPH_NAME = "chat_graph" as const;

/**
 * Input for chat graph execution.
 * graphVersion is passed at invocation time (e.g., from build-info).
 */
export interface ChatGraphInput {
  /** Conversation messages */
  readonly messages: Message[];
  /** Model to use */
  readonly model: string;
  /** Graph run ID (generated by facade) */
  readonly graphRunId: string;
  /** Graph version - git SHA passed from build-info at invocation */
  readonly graphVersion: string;
}

/**
 * Dependencies for chat graph.
 * Injected at execution time (no direct adapter imports).
 */
export interface ChatGraphDeps {
  /** LLM service for completions */
  readonly llmService: LlmService;
  /** Tool runner for tool execution */
  readonly toolRunner: ToolRunner;
  /** Callback to emit AiEvents */
  readonly emit: (event: AiEvent) => void;
}

/**
 * Execute the chat graph.
 *
 * P1 Skeleton: Currently a pass-through placeholder.
 * Future: Multi-step reasoning with tool use.
 *
 * @param input - Graph input
 * @param deps - Injected dependencies
 * @returns Async generator of AiEvents
 */
export async function* executeChatGraph(
  _input: ChatGraphInput,
  _deps: ChatGraphDeps
): AsyncGenerator<AiEvent> {
  // P1 Skeleton: Graph execution will be implemented when LangGraph is integrated
  // For now, direct LLM streaming is handled by ai_runtime.ts

  // This generator is a placeholder that yields nothing
  // Real implementation will:
  // 1. Run LLM with tool definitions
  // 2. When tool_call received, use toolRunner.exec()
  // 3. Feed tool results back to LLM
  // 4. Continue until completion

  // Satisfy TypeScript that this is a generator
  if (false as boolean) {
    yield { type: "done" as const };
  }
}
